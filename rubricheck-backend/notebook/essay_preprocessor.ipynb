{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "288a197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import sys\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "624bd0ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRubriCheck Essay Preprocessor\\n---------------------------------\\nA modular Python pipeline to prepare student essays for rubric-based grading.\\n\\nFeatures\\n- Language detection (with optional auto-translation hook)\\n- Optional PII minimization (redaction + reversible mapping)\\n- Section/paragraph parsing with stable indices\\n- Chunking with paragraph-level overlap for long essays\\n- Quote detection (inline & block) and quote ratio\\n- Metadata extraction (word/sentence counts, readability, section headers)\\n- Structured JSON-like output via dataclasses\\n\\nOptional dependencies (auto-detected if installed):\\n    pip install langdetect fasttext textstat spacy\\n    python -m spacy download en_core_web_sm\\n\\nNotes\\n- Translation is implemented via pluggable interface; default is NoOp.\\n- Readability prefers textstat when available; otherwise uses fallbacks.\\n- PII redaction uses regex for emails/phones and spaCy for PERSON/ORG when available.\\n\\nAuthor: RubriCheck\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "RubriCheck Essay Preprocessor\n",
    "---------------------------------\n",
    "A modular Python pipeline to prepare student essays for rubric-based grading.\n",
    "\n",
    "Features\n",
    "- Language detection (with optional auto-translation hook)\n",
    "- Optional PII minimization (redaction + reversible mapping)\n",
    "- Section/paragraph parsing with stable indices\n",
    "- Chunking with paragraph-level overlap for long essays\n",
    "- Quote detection (inline & block) and quote ratio\n",
    "- Metadata extraction (word/sentence counts, readability, section headers)\n",
    "- Structured JSON-like output via dataclasses\n",
    "\n",
    "Optional dependencies (auto-detected if installed):\n",
    "    pip install langdetect fasttext textstat spacy\n",
    "    python -m spacy download en_core_web_sm\n",
    "\n",
    "Notes\n",
    "- Translation is implemented via pluggable interface; default is NoOp.\n",
    "- Readability prefers textstat when available; otherwise uses fallbacks.\n",
    "- PII redaction uses regex for emails/phones and spaCy for PERSON/ORG when available.\n",
    "\n",
    "Author: RubriCheck\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f548a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Optional Dependency Detection\n",
    "# ------------------------------\n",
    "try:\n",
    "    from langdetect import detect as _ld_detect  # type: ignore\n",
    "except Exception:\n",
    "    _ld_detect = None\n",
    "\n",
    "try:\n",
    "    import fasttext  # type: ignore\n",
    "    _FASTTEXT_MODEL = None\n",
    "except Exception:\n",
    "    fasttext = None\n",
    "    _FASTTEXT_MODEL = None\n",
    "\n",
    "try:\n",
    "    import textstat  # type: ignore\n",
    "except Exception:\n",
    "    textstat = None\n",
    "\n",
    "try:\n",
    "    import spacy  # type: ignore\n",
    "    _SPACY_NLP = None\n",
    "except Exception:\n",
    "    spacy = None\n",
    "    _SPACY_NLP = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bec5182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Data Models\n",
    "# ------------------------------\n",
    "@dataclass\n",
    "class Section:\n",
    "    title: str\n",
    "    char_start: int\n",
    "    char_end: int\n",
    "\n",
    "@dataclass\n",
    "class Paragraph:\n",
    "    id: int\n",
    "    char_start: int\n",
    "    char_end: int\n",
    "    text: str\n",
    "\n",
    "@dataclass\n",
    "class QuoteSpan:\n",
    "    kind: str  # \"inline\" or \"block\"\n",
    "    char_start: int\n",
    "    char_end: int\n",
    "    text: str\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    id: int\n",
    "    paragraph_ids: List[int]\n",
    "    char_start: int\n",
    "    char_end: int\n",
    "    text: str\n",
    "\n",
    "@dataclass\n",
    "class Readability:\n",
    "    flesch_reading_ease: Optional[float] = None\n",
    "    flesch_kincaid_grade: Optional[float] = None\n",
    "    gunning_fog: Optional[float] = None\n",
    "    automated_readability_index: Optional[float] = None\n",
    "    coleman_liau_index: Optional[float] = None\n",
    "\n",
    "@dataclass\n",
    "class Metadata:\n",
    "    language_detected: str\n",
    "    translated: bool\n",
    "    target_language: str\n",
    "    word_count: int\n",
    "    sentence_count: int\n",
    "    char_count: int\n",
    "    quote_char_ratio: float\n",
    "    readability: Readability\n",
    "    sections: List[Section]\n",
    "\n",
    "@dataclass\n",
    "class PIIItem:\n",
    "    kind: str\n",
    "    original: str\n",
    "    replacement: str\n",
    "    char_start: int\n",
    "    char_end: int\n",
    "\n",
    "@dataclass\n",
    "class ProcessedEssay:\n",
    "    original_language: str\n",
    "    language: str\n",
    "    translated: bool\n",
    "    pii_redacted: bool\n",
    "    pii_map: List[PIIItem]\n",
    "    metadata: Metadata\n",
    "    paragraphs: List[Paragraph]\n",
    "    chunks: List[Chunk]\n",
    "    quotes: List[QuoteSpan]\n",
    "    warnings: List[str] = field(default_factory=list)\n",
    "\n",
    "    def to_json(self, indent: int = 2) -> str:\n",
    "        def encode(obj: Any):\n",
    "            if hasattr(obj, \"__dict__\"):\n",
    "                return asdict(obj)\n",
    "            if isinstance(obj, (set,)):\n",
    "                return list(obj)\n",
    "            raise TypeError(f\"Type not serializable: {type(obj)}\")\n",
    "        return json.dumps(asdict(self), indent=indent, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "268c9cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# Utilities\n",
    "# ------------------------------\n",
    "_WORD_RE = re.compile(r\"\\b\\w+\\b\", re.UNICODE)\n",
    "_SENT_RE = re.compile(r\"(?<!\\w\\.[a-z])(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!|\\n)\\s+\")\n",
    "# We will do a simpler sentence split to avoid heavy deps\n",
    "_SIMPLE_SENT_RE = re.compile(r\"[^.!?\\n]+[.!?]?\\n?\", re.MULTILINE)\n",
    "\n",
    "_EMAIL_RE = re.compile(r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\")\n",
    "_PHONE_RE = re.compile(r\"(?:\\+?\\d{1,3}[\\s.-]?)?(?:\\(?\\d{3}\\)?[\\s.-]?)?\\d{3}[\\s.-]?\\d{4}\")\n",
    "_URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
    "\n",
    "_INLINE_QUOTES_RE = re.compile(\n",
    "    r\"(\\“.*?\\”|\\\"[^\\\"]{3,}?\\\"|\\‘.*?\\’|\\'[^\\']{3,}?\\')\",\n",
    "    re.DOTALL,\n",
    ")\n",
    "_BLOCKQUOTE_LINE_RE = re.compile(r\"^\\s{0,3}>\\s?.+\", re.MULTILINE)\n",
    "\n",
    "_HEADER_PATTERNS = [\n",
    "    re.compile(r\"^\\s*#{1,6}\\s*(.+)$\", re.MULTILINE),  # Markdown\n",
    "    re.compile(r\"^\\s*(\\d+(?:\\.\\d+)*)\\s+(.{2,80})$\", re.MULTILINE),  # 1. Intro\n",
    "    re.compile(r\"^\\s*([A-Z][A-Z\\s]{3,})$\", re.MULTILINE),  # ALL CAPS LINES\n",
    "]\n",
    "_KNOWN_HEADERS = {\n",
    "    \"abstract\", \"introduction\", \"background\", \"literature review\",\n",
    "    \"methods\", \"methodology\", \"results\", \"analysis\", \"discussion\",\n",
    "    \"conclusion\", \"limitations\", \"future work\", \"references\",\n",
    "    \"works cited\", \"acknowledgments\",\n",
    "}\n",
    "\n",
    "\n",
    "def _simple_words(text: str) -> List[str]:\n",
    "    return re.findall(r\"[A-Za-zÀ-ÖØ-öø-ÿ']+\", text)\n",
    "\n",
    "\n",
    "def _count_sentences(text: str) -> int:\n",
    "    sentences = [s.strip() for s in _SIMPLE_SENT_RE.findall(text) if s.strip()]\n",
    "    return max(1, len(sentences))\n",
    "\n",
    "\n",
    "def _count_syllables(word: str) -> int:\n",
    "    # Very rough heuristic syllable counter for fallback readability\n",
    "    word = word.lower()\n",
    "    if not word:\n",
    "        return 0\n",
    "    vowels = \"aeiouyà-öø-ÿ\"\n",
    "    count = 0\n",
    "    prev_is_vowel = False\n",
    "    for ch in word:\n",
    "        is_vowel = ch in vowels\n",
    "        if is_vowel and not prev_is_vowel:\n",
    "            count += 1\n",
    "        prev_is_vowel = is_vowel\n",
    "    if word.endswith(\"e\") and count > 1:\n",
    "        count -= 1\n",
    "    return max(1, count)\n",
    "\n",
    "\n",
    "def _safe_div(a: float, b: float) -> float:\n",
    "    return a / b if b else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9241b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Language Detection & Translation\n",
    "# ------------------------------\n",
    "class LanguageDetector:\n",
    "    def __init__(self):\n",
    "        self.fasttext_model = None\n",
    "        if fasttext is not None:\n",
    "            try:\n",
    "                # You may load a lid.176.ftz model here if available.\n",
    "                # self.fasttext_model = fasttext.load_model(\"lid.176.ftz\")\n",
    "                self.fasttext_model = None\n",
    "            except Exception:\n",
    "                self.fasttext_model = None\n",
    "\n",
    "    def detect(self, text: str) -> str:\n",
    "        # Prioritize langdetect if available as zero-setup\n",
    "        if _ld_detect is not None:\n",
    "            try:\n",
    "                return _ld_detect(text)\n",
    "            except Exception:\n",
    "                pass\n",
    "        # Fallback crude heuristic: assume English if mostly ASCII\n",
    "        ascii_ratio = _safe_div(sum(1 for c in text if ord(c) < 128), max(1, len(text)))\n",
    "        return \"en\" if ascii_ratio > 0.9 else \"unknown\"\n",
    "\n",
    "\n",
    "class Translator:\n",
    "    def translate(self, text: str, target_language: str) -> Tuple[str, bool]:\n",
    "        \"\"\"Return (translated_text, changed?). Override in subclasses.\"\"\"\n",
    "        return text, False\n",
    "\n",
    "\n",
    "class NoOpTranslator(Translator):\n",
    "    pass\n",
    "\n",
    "\n",
    "class OpenAITranslator(Translator):\n",
    "    \"\"\"Stub for OpenAI translation—implement with your API client externally.\"\"\"\n",
    "    def __init__(self, client=None, model: str = \"gpt-4o-mini\"):\n",
    "        self.client = client\n",
    "        self.model = model\n",
    "\n",
    "    def translate(self, text: str, target_language: str) -> Tuple[str, bool]:\n",
    "        # Pseudocode:\n",
    "        # resp = self.client.chat.completions.create(\n",
    "        #     model=self.model,\n",
    "        #     messages=[{\"role\":\"system\",\"content\":f\"Translate to {target_language}.\"},\n",
    "        #               {\"role\":\"user\",\"content\":text}],\n",
    "        #     temperature=0\n",
    "        # )\n",
    "        # t = resp.choices[0].message.content\n",
    "        # return t, (t.strip() != text.strip())\n",
    "        return text, False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bac47fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# PII Minimization\n",
    "# ------------------------------\n",
    "class PIIRedactor:\n",
    "    def __init__(self, enable_spacy: bool = True):\n",
    "        self.use_spacy = enable_spacy and (spacy is not None)\n",
    "        self._nlp = None\n",
    "        if self.use_spacy:\n",
    "            global _SPACY_NLP\n",
    "            if _SPACY_NLP is None:\n",
    "                try:\n",
    "                    _SPACY_NLP = spacy.load(\"en_core_web_sm\")\n",
    "                except Exception:\n",
    "                    _SPACY_NLP = None\n",
    "            self._nlp = _SPACY_NLP\n",
    "\n",
    "    def _spacy_entities(self, text: str) -> List[Tuple[str, int, int, str]]:\n",
    "        ents: List[Tuple[str, int, int, str]] = []\n",
    "        if not self._nlp:\n",
    "            return ents\n",
    "        try:\n",
    "            doc = self._nlp(text)\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ in {\"PERSON\", \"ORG\"}:\n",
    "                    ents.append((ent.text, ent.start_char, ent.end_char, ent.label_))\n",
    "        except Exception:\n",
    "            pass\n",
    "        return ents\n",
    "\n",
    "    def redact(self, text: str) -> Tuple[str, List[PIIItem]]:\n",
    "        mappings: List[PIIItem] = []\n",
    "        # First pass: regex-based (emails, phones, URLs)\n",
    "        def replace_with(tag: str):\n",
    "            counter = {\"n\": 0}\n",
    "            def _repl(m: re.Match) -> str:\n",
    "                counter[\"n\"] += 1\n",
    "                placeholder = f\"[REDACTED_{tag}_{counter['n']}]\"\n",
    "                mappings.append(PIIItem(tag.lower(), m.group(0), placeholder, m.start(), m.end()))\n",
    "                return placeholder\n",
    "            return _repl\n",
    "\n",
    "        text = _EMAIL_RE.sub(replace_with(\"EMAIL\"), text)\n",
    "        text = _PHONE_RE.sub(replace_with(\"PHONE\"), text)\n",
    "        text = _URL_RE.sub(replace_with(\"URL\"), text)\n",
    "\n",
    "        # Second pass: spaCy named entities (PERSON/ORG) if available\n",
    "        if self._nlp is not None:\n",
    "            # We must map spans carefully after prior replacements; recompute indices by scanning\n",
    "            ents = self._spacy_entities(text)\n",
    "            # Replace from end to start to keep offsets valid\n",
    "            ents_sorted = sorted(ents, key=lambda x: x[1], reverse=True)\n",
    "            idx = 0\n",
    "            for original, start, end, label in ents_sorted:\n",
    "                idx += 1\n",
    "                placeholder = f\"[REDACTED_{label}_{idx}]\"\n",
    "                mappings.append(PIIItem(label.lower(), original, placeholder, start, end))\n",
    "                text = text[:start] + placeholder + text[end:]\n",
    "        return text, mappings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f801c67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# Section & Paragraph Parsing\n",
    "# ------------------------------\n",
    "class StructureParser:\n",
    "    @staticmethod\n",
    "    def find_sections(text: str) -> List[Section]:\n",
    "        found: List[Section] = []\n",
    "        # Pattern-based headers\n",
    "        for pat in _HEADER_PATTERNS:\n",
    "            for m in pat.finditer(text):\n",
    "                title = m.group(1).strip() if m.lastindex else m.group(0).strip()\n",
    "                start = m.start()\n",
    "                # Section end is unknown here; consumer can infer by next start\n",
    "                found.append(Section(title=title, char_start=start, char_end=m.end()))\n",
    "        # Known headings (case-insensitive) as standalone lines\n",
    "        lines = text.splitlines(keepends=True)\n",
    "        offset = 0\n",
    "        for line in lines:\n",
    "            clean = line.strip().lower()\n",
    "            if clean in _KNOWN_HEADERS:\n",
    "                found.append(Section(title=line.strip(), char_start=offset, char_end=offset + len(line)))\n",
    "            offset += len(line)\n",
    "        # Deduplicate by (start,end)\n",
    "        uniq = {(s.char_start, s.char_end): s for s in found}\n",
    "        return sorted(uniq.values(), key=lambda s: s.char_start)\n",
    "\n",
    "    @staticmethod\n",
    "    def split_paragraphs(text: str) -> List[Paragraph]:\n",
    "        paras: List[Paragraph] = []\n",
    "        # Split on blank lines (>=2 newlines) while preserving char spans\n",
    "        parts = re.split(r\"\\n\\s*\\n+\", text)\n",
    "        cursor = 0\n",
    "        pid = 0\n",
    "        for part in parts:\n",
    "            # Find this part in the original text starting at cursor\n",
    "            idx = text.find(part, cursor)\n",
    "            if idx == -1:\n",
    "                idx = cursor\n",
    "            para_text = part.strip(\"\\n\")\n",
    "            char_start = idx\n",
    "            char_end = idx + len(part)\n",
    "            paras.append(Paragraph(id=pid, char_start=char_start, char_end=char_end, text=para_text))\n",
    "            pid += 1\n",
    "            cursor = char_end\n",
    "        return paras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb787f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# Quote Detection\n",
    "# ------------------------------\n",
    "class QuoteDetector:\n",
    "    @staticmethod\n",
    "    def detect(text: str) -> List[QuoteSpan]:\n",
    "        quotes: List[QuoteSpan] = []\n",
    "        # Inline quotes\n",
    "        for m in _INLINE_QUOTES_RE.finditer(text):\n",
    "            quotes.append(QuoteSpan(kind=\"inline\", char_start=m.start(), char_end=m.end(), text=m.group(0)))\n",
    "        # Blockquotes (markdown-style)\n",
    "        for m in _BLOCKQUOTE_LINE_RE.finditer(text):\n",
    "            start = m.start()\n",
    "            # Extend contiguous blockquote lines\n",
    "            end = m.end()\n",
    "            quotes.append(QuoteSpan(kind=\"block\", char_start=start, char_end=end, text=m.group(0)))\n",
    "        # Merge overlapping spans\n",
    "        quotes = QuoteDetector._merge_overlaps(quotes)\n",
    "        return quotes\n",
    "\n",
    "    @staticmethod\n",
    "    def _merge_overlaps(spans: List[QuoteSpan]) -> List[QuoteSpan]:\n",
    "        if not spans:\n",
    "            return spans\n",
    "        spans = sorted(spans, key=lambda s: s.char_start)\n",
    "        merged: List[QuoteSpan] = []\n",
    "        cur = spans[0]\n",
    "        for s in spans[1:]:\n",
    "            if s.char_start <= cur.char_end:\n",
    "                cur.char_end = max(cur.char_end, s.char_end)\n",
    "                cur.text = \"\"  # omit merged text to save memory\n",
    "                cur.kind = cur.kind if cur.kind == s.kind else \"block\"\n",
    "            else:\n",
    "                merged.append(cur)\n",
    "                cur = s\n",
    "        merged.append(cur)\n",
    "        return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a362e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Chunking\n",
    "# ------------------------------\n",
    "@dataclass\n",
    "class ChunkingConfig:\n",
    "    max_paragraphs: int = 6\n",
    "    overlap_paragraphs: int = 1\n",
    "\n",
    "\n",
    "class Chunker:\n",
    "    def __init__(self, cfg: ChunkingConfig):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def make_chunks(self, paragraphs: List[Paragraph]) -> List[Chunk]:\n",
    "        chunks: List[Chunk] = []\n",
    "        if not paragraphs:\n",
    "            return chunks\n",
    "        i = 0\n",
    "        cid = 0\n",
    "        while i < len(paragraphs):\n",
    "            start_i = i\n",
    "            end_i = min(len(paragraphs), i + self.cfg.max_paragraphs)\n",
    "            para_slice = paragraphs[start_i:end_i]\n",
    "            text = \"\\n\\n\".join(p.text for p in para_slice)\n",
    "            char_start = para_slice[0].char_start\n",
    "            char_end = para_slice[-1].char_end\n",
    "            chunks.append(\n",
    "                Chunk(\n",
    "                    id=cid,\n",
    "                    paragraph_ids=[p.id for p in para_slice],\n",
    "                    char_start=char_start,\n",
    "                    char_end=char_end,\n",
    "                    text=text,\n",
    "                )\n",
    "            )\n",
    "            cid += 1\n",
    "            if end_i >= len(paragraphs):\n",
    "                break\n",
    "            # Overlap by N paragraphs\n",
    "            i = end_i - self.cfg.overlap_paragraphs\n",
    "            if i <= start_i:  # ensure progress\n",
    "                i = end_i\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1804bc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "# Metadata Extraction\n",
    "# ------------------------------\n",
    "class MetadataExtractor:\n",
    "    @staticmethod\n",
    "    def word_count(text: str) -> int:\n",
    "        return len(_simple_words(text))\n",
    "\n",
    "    @staticmethod\n",
    "    def sentence_count(text: str) -> int:\n",
    "        return _count_sentences(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def readability(text: str) -> Readability:\n",
    "        if textstat is not None:\n",
    "            try:\n",
    "                return Readability(\n",
    "                    flesch_reading_ease=textstat.flesch_reading_ease(text),\n",
    "                    flesch_kincaid_grade=textstat.flesch_kincaid_grade(text),\n",
    "                    gunning_fog=textstat.gunning_fog(text),\n",
    "                    automated_readability_index=textstat.automated_readability_index(text),\n",
    "                    coleman_liau_index=textstat.coleman_liau_index(text),\n",
    "                )\n",
    "            except Exception:\n",
    "                pass\n",
    "        # Fallbacks\n",
    "        words = _simple_words(text)\n",
    "        sentences = MetadataExtractor.sentence_count(text)\n",
    "        chars = sum(len(w) for w in words)\n",
    "        syllables = sum(_count_syllables(w) for w in words)\n",
    "        # Basic formulas\n",
    "        fre = 206.835 - 1.015 * _safe_div(len(words), sentences) - 84.6 * _safe_div(syllables, len(words))\n",
    "        fk = 0.39 * _safe_div(len(words), sentences) + 11.8 * _safe_div(syllables, len(words)) - 15.59\n",
    "        ari = 4.71 * _safe_div(chars, len(words)) + 0.5 * _safe_div(len(words), sentences) - 21.43\n",
    "        cli = 0.0588 * (100 * _safe_div(chars, len(words))) - 0.296 * (100 * _safe_div(sentences, len(words))) - 15.8\n",
    "        # Gunning Fog fallback\n",
    "        complex_words = sum(1 for w in words if _count_syllables(w) >= 3)\n",
    "        gf = 0.4 * (_safe_div(len(words), sentences) + 100 * _safe_div(complex_words, len(words)))\n",
    "        return Readability(\n",
    "            flesch_reading_ease=fre,\n",
    "            flesch_kincaid_grade=fk,\n",
    "            gunning_fog=gf,\n",
    "            automated_readability_index=ari,\n",
    "            coleman_liau_index=cli,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def sections(text: str) -> List[Section]:\n",
    "        return StructureParser.find_sections(text)\n",
    "\n",
    "    @staticmethod\n",
    "    def quote_ratio(text: str, quotes: List[QuoteSpan]) -> float:\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        quoted_chars = sum(q.char_end - q.char_start for q in quotes)\n",
    "        return _safe_div(quoted_chars, len(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f35e5fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Options & Orchestrator\n",
    "# ------------------------------\n",
    "@dataclass\n",
    "class PreprocessOptions:\n",
    "    target_language: str = \"en\"\n",
    "    translate_non_english: bool = True\n",
    "    redact_pii: bool = True\n",
    "    chunk_max_paragraphs: int = 6\n",
    "    chunk_overlap_paragraphs: int = 1\n",
    "\n",
    "\n",
    "class EssayPreprocessor:\n",
    "    def __init__(self, translator: Optional[Translator] = None, pii_spacy: bool = True):\n",
    "        self.lang_detector = LanguageDetector()\n",
    "        self.translator = translator or NoOpTranslator()\n",
    "        self.pii = PIIRedactor(enable_spacy=pii_spacy)\n",
    "\n",
    "    def run(self, text: str, opts: Optional[PreprocessOptions] = None) -> ProcessedEssay:\n",
    "        opts = opts or PreprocessOptions()\n",
    "        warnings: List[str] = []\n",
    "\n",
    "        # Language detection\n",
    "        detected_lang = self.lang_detector.detect(text)\n",
    "        working_text = text\n",
    "        did_translate = False\n",
    "        if opts.translate_non_english and detected_lang and detected_lang != opts.target_language and detected_lang != \"unknown\":\n",
    "            try:\n",
    "                working_text, did_translate = self.translator.translate(working_text, opts.target_language)\n",
    "            except Exception as e:\n",
    "                warnings.append(f\"Translation failed: {e}\")\n",
    "\n",
    "        # PII Minimization (student-facing)\n",
    "        pii_map: List[PIIItem] = []\n",
    "        if opts.redact_pii:\n",
    "            try:\n",
    "                working_text, pii_map = self.pii.redact(working_text)\n",
    "            except Exception as e:\n",
    "                warnings.append(f\"PII redaction failed: {e}\")\n",
    "\n",
    "        # Structure: sections + paragraphs\n",
    "        sections = MetadataExtractor.sections(working_text)\n",
    "        paragraphs = StructureParser.split_paragraphs(working_text)\n",
    "\n",
    "        # Quote detection\n",
    "        quotes = QuoteDetector.detect(working_text)\n",
    "\n",
    "        # Chunking\n",
    "        chunker = Chunker(ChunkingConfig(\n",
    "            max_paragraphs=opts.chunk_max_paragraphs,\n",
    "            overlap_paragraphs=opts.chunk_overlap_paragraphs,\n",
    "        ))\n",
    "        chunks = chunker.make_chunks(paragraphs)\n",
    "\n",
    "        # Metadata\n",
    "        wc = MetadataExtractor.word_count(working_text)\n",
    "        sc = MetadataExtractor.sentence_count(working_text)\n",
    "        cc = len(working_text)\n",
    "        quote_ratio = MetadataExtractor.quote_ratio(working_text, quotes)\n",
    "        readability = MetadataExtractor.readability(working_text)\n",
    "\n",
    "        meta = Metadata(\n",
    "            language_detected=detected_lang or \"unknown\",\n",
    "            translated=did_translate,\n",
    "            target_language=opts.target_language,\n",
    "            word_count=wc,\n",
    "            sentence_count=sc,\n",
    "            char_count=cc,\n",
    "            quote_char_ratio=quote_ratio,\n",
    "            readability=readability,\n",
    "            sections=sections,\n",
    "        )\n",
    "\n",
    "        processed = ProcessedEssay(\n",
    "            original_language=detected_lang or \"unknown\",\n",
    "            language=opts.target_language if did_translate else detected_lang or \"unknown\",\n",
    "            translated=did_translate,\n",
    "            pii_redacted=opts.redact_pii,\n",
    "            pii_map=pii_map,\n",
    "            metadata=meta,\n",
    "            paragraphs=paragraphs,\n",
    "            chunks=chunks,\n",
    "            quotes=quotes,\n",
    "            warnings=warnings,\n",
    "        )\n",
    "        return processed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72618367",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEMO_TEXT = \"\"\"\n",
    "\n",
    "Moss, Howard (1922-1987)   Howard Moss was an important practitioner of formal verse in the mid-twentieth century. He also had an uncanny ability to envision—and thereby in his poems to transform—nature into the environment created by humanity, bringing it into the realm of civilization; he strove to formalize nature, in keeping with his view of what poetry should be. He once remarked, \"What my poems are really about […] is the experience of hovering between the forms of nature and the forms of art\" (Leiter 29). Moss set an example for his generation among poets who believed in explicit order, and for later poets who have identified themselves with NEW FORMALISM. He was also the poetry editor at The New Yorker magazine from 1950 until shortly before his death, a position that allowed him to orchestrate much of mainstream American writing.\n",
    "\n",
    "Moss was born and raised in New York City. In 1942, he won Poetry magazine’s Janet Sewall David Award for his own poetry. His first book was published in 1946. He was inducted into the American Academy and Institute of Arts and Letters in 1968, and won the National Book Award for poetry for his Selected Poems in 1971. His New Selected Poems (1984) was awarded the Lenore Marshall / Nation Poetry Prize in 1986, a year when he received a fellowship from the Academy of American Poets.\n",
    "\n",
    "All of Moss's work possesses a subtle finish. His early and middle poems are end-rhymed and metered, his later work freer—but all of it has a striking regularity of meter and tone. The prevalent themes in Moss's work involve fundamental issues such as change in life, human relationships, loss, and death. He writes ably of \"the difficulty of love, the decay of the body, the passing of time, and the inevitability of death,\" all set against \"the inexhaustible beauty of the natural world,\" as Dana Gioia has observed (102). He is, in fact, a great elegist who can portray attachment and loss with stunning acuity through graphic simplicity and bitter irony.\n",
    "\n",
    "In \"Elegy for My Sister\" (1980) he painstakingly details his sister’s fatal disease and her struggle to cope with it. Trying to rise from her bed, her leg breaks \"simply by standing up\"; her bones have been \"[m]elted into a kind of eggshell sawdust\" by chemotherapy. His metaphors go beyond physical distress to show the plight of the soul. And in \"Elegy for My Father\" (1954) intense pain, dying and separation are made vivid through paradox. His father, for example, is freed from life by his pain, a \"double-dealing enemy.\"\n",
    "\n",
    "Moss's finely crafted verse is matched by his willingness to account for the peripatetic and otherwise insignificant details of living, making them at times monumental. In his work the truth peeks out through artifice.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a47124a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Metadata ==\n",
      "Detected language: en\n",
      "Translated: False\n",
      "Word count: 460\n",
      "Sentences: 23\n",
      "Chars: 2757\n",
      "Quote ratio: 0.215\n",
      "Readability:\n",
      "  Flesch Reading Ease: 51.54282608695655\n",
      "  Flesch-Kincaid Grade: 11.038695652173914\n",
      "\n",
      "== Sections ==\n",
      "\n",
      "== Paragraphs ==\n",
      " #0 [0:0] -> ''\n",
      " #1 [2:861] -> 'Moss, Howard ([REDACTED_PHONE_1])   Howard Moss was an impor'\n",
      " #2 [863:1348] -> 'Moss was born and raised in New York City. In 1942, he won P'\n",
      " #3 [1350:2008] -> \"All of Moss's work possesses a subtle finish. His early and \"\n",
      " #4 [2010:2536] -> 'In \"Elegy for My Sister\" (1980) he painstakingly details his'\n",
      " #5 [2538:2757] -> \"Moss's finely crafted verse is matched by his willingness to\"\n",
      "\n",
      "== Chunks ==\n",
      " * Chunk 0 paras=[0, 1, 2, 3, 4, 5] [0:2757] len=2756\n",
      "\n",
      "== Quotes ==\n",
      " - inline [400:515] -> '\"What my poems are really about […] is the experience of hov'\n",
      " - inline [1361:1560] -> \"'s work possesses a subtle finish. His early and middle poem\"\n",
      " - inline [1674:1775] -> '\"the difficulty of love, the decay of the body, the passing '\n",
      " - inline [1792:1840] -> '\"the inexhaustible beauty of the natural world,\"'\n",
      " - inline [2013:2034] -> '\"Elegy for My Sister\"'\n",
      " - inline [2172:2195] -> '\"simply by standing up\"'\n",
      " - inline [2217:2259] -> '\"[m]elted into a kind of eggshell sawdust\"'\n",
      " - inline [2358:2379] -> '\"Elegy for My Father\"'\n",
      " - inline [2513:2536] -> '\"double-dealing enemy.\"'\n",
      "\n",
      "== PII Map ==\n",
      " - phone: '1922-1987' -> [REDACTED_PHONE_1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# ------------------------------\n",
    "# CLI / Demo\n",
    "# ------------------------------\n",
    "\n",
    "def _print_summary(proc: ProcessedEssay) -> None:\n",
    "    print(\"== Metadata ==\")\n",
    "    print(f\"Detected language: {proc.metadata.language_detected}\")\n",
    "    print(f\"Translated: {proc.metadata.translated}\")\n",
    "    print(f\"Word count: {proc.metadata.word_count}\")\n",
    "    print(f\"Sentences: {proc.metadata.sentence_count}\")\n",
    "    print(f\"Chars: {proc.metadata.char_count}\")\n",
    "    print(f\"Quote ratio: {proc.metadata.quote_char_ratio:.3f}\")\n",
    "    print(\"Readability:\")\n",
    "    print(f\"  Flesch Reading Ease: {proc.metadata.readability.flesch_reading_ease}\")\n",
    "    print(f\"  Flesch-Kincaid Grade: {proc.metadata.readability.flesch_kincaid_grade}\")\n",
    "\n",
    "    print(\"\\n== Sections ==\")\n",
    "    for s in proc.metadata.sections:\n",
    "        print(f\" - {s.title} [{s.char_start}:{s.char_end}]\")\n",
    "\n",
    "    print(\"\\n== Paragraphs ==\")\n",
    "    for p in proc.paragraphs:\n",
    "        print(f\" #{p.id} [{p.char_start}:{p.char_end}] -> {p.text[:60]!r}\")\n",
    "\n",
    "    print(\"\\n== Chunks ==\")\n",
    "    for c in proc.chunks:\n",
    "        print(f\" * Chunk {c.id} paras={c.paragraph_ids} [{c.char_start}:{c.char_end}] len={len(c.text)}\")\n",
    "\n",
    "    print(\"\\n== Quotes ==\")\n",
    "    for q in proc.quotes:\n",
    "        print(f\" - {q.kind} [{q.char_start}:{q.char_end}] -> {q.text[:60]!r}\")\n",
    "\n",
    "    if proc.pii_redacted and proc.pii_map:\n",
    "        print(\"\\n== PII Map ==\")\n",
    "        for item in proc.pii_map[:10]:\n",
    "            print(f\" - {item.kind}: {item.original!r} -> {item.replacement}\")\n",
    "\n",
    "    if proc.warnings:\n",
    "        print(\"\\n== Warnings ==\")\n",
    "        for w in proc.warnings:\n",
    "            print(\" -\", w)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"RubriCheck Essay Preprocessor\")\n",
    "    parser.add_argument(\"--file\", type=str, help=\"Path to essay text file\", default=None)\n",
    "    parser.add_argument(\"--no-redact\", action=\"store_true\", help=\"Disable PII redaction\")\n",
    "    parser.add_argument(\"--no-translate\", action=\"store_true\", help=\"Disable translation of non-English\")\n",
    "    parser.add_argument(\"--json\", action=\"store_true\", help=\"Print JSON output\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # if args.file:\n",
    "    #     print(args.file)\n",
    "    #     with open(args.file, \"r\", encoding=\"utf-8\") as f:\n",
    "    #         text = f.read()\n",
    "    # else:\n",
    "    #     text = DEMO_TEXT\n",
    "    #     print(text)\n",
    "\n",
    "    text = DEMO_TEXT\n",
    "    pre = EssayPreprocessor(translator=NoOpTranslator())\n",
    "    proc = pre.run(text, PreprocessOptions(\n",
    "        redact_pii=not args.no_redact,\n",
    "        translate_non_english=not args.no_translate,\n",
    "    ))\n",
    "\n",
    "    if args.json:\n",
    "        print(proc.to_json())\n",
    "    else:\n",
    "        _print_summary(proc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9824788d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68912ca1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
