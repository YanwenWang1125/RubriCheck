{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a36d168e-7208-4352-80b9-02db79542316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rubric_parser.py\n",
    "# --------------------------------------------------------------------\n",
    "# Ingest TXT/DOCX/PDF/Image ‚Üí extract text ‚Üí LLM parse (Structured Outputs)\n",
    "# ‚Üí local JSON Schema validation ‚Üí return normalized rubric JSON\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "import os, io, re, json, tempfile, mimetypes, math\n",
    "from typing import Tuple, Dict, Any, Optional, List\n",
    "\n",
    "# --- File extraction deps\n",
    "import fitz  # PyMuPDF\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import docx2txt\n",
    "\n",
    "# --- OpenAI (Responses API with Structured Outputs)\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- Validation\n",
    "from jsonschema import Draft7Validator\n",
    "\n",
    "# =========================\n",
    "# JSON SCHEMA (Structured)\n",
    "# =========================\n",
    "\n",
    "RUBRIC_JSON_SCHEMA: Dict[str, Any] = {\n",
    "    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "    \"title\": \"RubricSchema\",\n",
    "    \"type\": \"object\",\n",
    "    \"additionalProperties\": False,\n",
    "    \"properties\": {\n",
    "        \"title\": {\"type\": [\"string\", \"null\"], \"maxLength\": 200},\n",
    "        \"scale\": {\n",
    "            \"type\": \"object\",\n",
    "            \"additionalProperties\": False,\n",
    "            \"properties\": {\n",
    "                \"type\": {\"type\": \"string\", \"enum\": [\"categorical\", \"numeric\"]},\n",
    "                \"levels\": {\n",
    "                    \"type\": [\"array\", \"null\"],\n",
    "                    \"items\": {\"type\": \"string\", \"minLength\": 1},\n",
    "                    \"minItems\": 1\n",
    "                },\n",
    "                \"min\": {\"type\": [\"number\", \"null\"]},\n",
    "                \"max\": {\"type\": [\"number\", \"null\"]},\n",
    "                \"original_levels\": {\n",
    "                    \"type\": [\"array\", \"null\"],\n",
    "                    \"items\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"synonyms\": {\n",
    "                    \"type\": [\"object\", \"null\"],\n",
    "                    \"additionalProperties\": {\"type\": \"string\"}\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"type\"]\n",
    "        },\n",
    "        \"criteria\": {\n",
    "            \"type\": \"array\",\n",
    "            \"minItems\": 1,\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"additionalProperties\": False,\n",
    "                \"properties\": {\n",
    "                    \"name\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 120},\n",
    "                    \"descriptor_by_level\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"additionalProperties\": {\"type\": \"string\"}\n",
    "                    },\n",
    "                    \"weight\": {\"type\": \"number\", \"exclusiveMinimum\": 0},\n",
    "                    \"evidence_hint\": {\"type\": [\"string\", \"null\"]},\n",
    "                    \"notes\": {\"type\": [\"string\", \"null\"]}\n",
    "                },\n",
    "                \"required\": [\"name\", \"descriptor_by_level\"]\n",
    "            }\n",
    "        },\n",
    "        \"notes\": {\"type\": [\"string\", \"null\"]},\n",
    "        \"source_parse\": {\n",
    "            \"type\": \"object\",\n",
    "            \"additionalProperties\": False,\n",
    "            \"properties\": {\n",
    "                \"method\": {\"type\": \"string\", \"enum\": [\"table\", \"narrative\", \"hybrid\", \"ocr\"]},\n",
    "                \"confidence\": {\"type\": \"number\", \"minimum\": 0.0, \"maximum\": 1.0},\n",
    "                \"warnings\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"},\n",
    "                    \"default\": []\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"method\", \"confidence\"]\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"scale\", \"criteria\", \"source_parse\"]\n",
    "}\n",
    "\n",
    "# Pre-compile validator for speed\n",
    "RUBRIC_VALIDATOR = Draft7Validator(RUBRIC_JSON_SCHEMA)\n",
    "\n",
    "# =========================\n",
    "# File ‚Üí text extraction\n",
    "# =========================\n",
    "\n",
    "IMG_EXT = {\".png\", \".jpg\", \".jpeg\", \".webp\", \".tif\", \".tiff\", \".bmp\"}\n",
    "\n",
    "def _deskew_and_binarize(pil_img: Image.Image) -> Image.Image:\n",
    "    \"\"\"Basic deskew + binarization to improve OCR.\"\"\"\n",
    "    img = np.array(pil_img.convert(\"L\"))  # grayscale\n",
    "    # threshold\n",
    "    _, th = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    # deskew\n",
    "    coords = np.column_stack(np.where(th == 0))\n",
    "    angle = 0.0\n",
    "    if coords.size > 0:\n",
    "        rect = cv2.minAreaRect(coords)\n",
    "        angle = rect[-1]\n",
    "        if angle < -45:\n",
    "            angle = -(90 + angle)\n",
    "        else:\n",
    "            angle = -angle\n",
    "    (h, w) = th.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)\n",
    "    rotated = cv2.warpAffine(th, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "    return Image.fromarray(rotated)\n",
    "\n",
    "def _ocr_pil_image(pil_img: Image.Image, lang: str = \"eng\") -> str:\n",
    "    proc = _deskew_and_binarize(pil_img)\n",
    "    return pytesseract.image_to_string(proc, lang=lang)\n",
    "\n",
    "def _extract_from_pdf(path: str) -> Tuple[str, str]:\n",
    "    \"\"\"Return (text, method). Try native text first; fallback to OCR if text looks empty.\"\"\"\n",
    "    doc = fitz.open(path)\n",
    "    texts = []\n",
    "    for p in doc:\n",
    "        txt = p.get_text(\"text\")\n",
    "        if txt:\n",
    "            texts.append(txt)\n",
    "    native_text = \"\\n\".join(texts).strip()\n",
    "    if len(native_text) >= 400 or (len(native_text) > 40 and len(texts) >= 1):\n",
    "        return native_text, \"table\" if \" | \" in native_text or re.search(r\"\\bPoints?\\b\", native_text, re.I) else \"narrative\"\n",
    "\n",
    "    # Fallback to OCR\n",
    "    pages = convert_from_path(path, dpi=300)\n",
    "    ocr_texts = []\n",
    "    for pil in pages:\n",
    "        ocr_texts.append(_ocr_pil_image(pil))\n",
    "    return \"\\n\".join(ocr_texts).strip(), \"ocr\"\n",
    "\n",
    "def _extract_from_image(path: str) -> Tuple[str, str]:\n",
    "    pil = Image.open(path)\n",
    "    return _ocr_pil_image(pil), \"ocr\"\n",
    "\n",
    "def _extract_from_docx(path: str) -> Tuple[str, str]:\n",
    "    return docx2txt.process(path) or \"\", \"narrative\"\n",
    "\n",
    "def _extract_from_txt(path: str) -> Tuple[str, str]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read(), \"narrative\"\n",
    "\n",
    "def extract_text_from_file(path: str) -> Tuple[str, str]:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        return _extract_from_pdf(path)\n",
    "    if ext == \".docx\":\n",
    "        return _extract_from_docx(path)\n",
    "    if ext in IMG_EXT:\n",
    "        return _extract_from_image(path)\n",
    "    return _extract_from_txt(path)\n",
    "\n",
    "# =========================\n",
    "# OpenAI call (Structured Outputs)\n",
    "# =========================\n",
    "\n",
    "def _build_messages(raw_text: str, parse_hint_method: str) -> list:\n",
    "    sys = (\n",
    "        \"You are a precise rubric parser. Convert the given rubric into strictly valid JSON that \"\n",
    "        \"conforms to the provided JSON Schema. Do not fabricate content. If information is missing, \"\n",
    "        \"omit it and write a warning. Preserve original level wording in descriptors; normalize level names.\"\n",
    "    )\n",
    "    usr = (\n",
    "        \"RAW_RUBRIC_TEXT:\\n```\\n\" + raw_text.strip() + \"\\n```\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        f\"- The extraction method was '{parse_hint_method}'. Set source_parse.method accordingly.\\n\"\n",
    "        \"- If levels like Excellent/Good/Fair/Poor are present, use them as categorical scale levels in best‚Üíworst order.\\n\"\n",
    "        \"- If a numeric points scale is present (e.g., 0‚Äì4), include scale.min/scale.max and set type='numeric'.\\n\"\n",
    "        \"- Parse weights when explicitly indicated (e.g., 'Clarity (30%)' ‚Üí weight=0.30); otherwise default to 1.0.\\n\"\n",
    "        \"- If any descriptor is missing for a level, omit that key and add a warning.\\n\"\n",
    "        \"- If multiple rubrics are present, parse the first major rubric and add a warning.\\n\"\n",
    "        \"- Output only the JSON. No extra text.\"\n",
    "    )\n",
    "    return [{\"role\": \"system\", \"content\": sys}, {\"role\": \"user\", \"content\": usr}]\n",
    "\n",
    "def _openai_client() -> OpenAI:\n",
    "    return OpenAI()  # reads OPENAI_API_KEY\n",
    "\n",
    "def parse_rubric_with_llm(raw_text: str, method_hint: str = \"narrative\", model: str = \"gpt-4.1-mini\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calls the OpenAI Responses API with Structured Outputs (json_schema) to get a strictly valid rubric JSON.\n",
    "    \"\"\"\n",
    "    client = _openai_client()\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=model,\n",
    "        input=_build_messages(raw_text, method_hint),\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"RubricSchema\",\n",
    "                \"strict\": True,\n",
    "                \"schema\": RUBRIC_JSON_SCHEMA\n",
    "            }\n",
    "        },\n",
    "        max_output_tokens=2048,\n",
    "    )\n",
    "    # With Structured Outputs, the SDK returns parsed JSON under .output[0].content[0].parsed (Python SDKs may vary).\n",
    "    # To be safe, fallback to .output_text then json.loads if needed.\n",
    "    try:\n",
    "        # Newer SDKs expose a convenience attribute:\n",
    "        parsed = response.output[0].content[0].parsed  # type: ignore[attr-defined]\n",
    "    except Exception:\n",
    "        parsed = json.loads(response.output_text)\n",
    "\n",
    "    return parsed\n",
    "\n",
    "# =========================\n",
    "# Validation & post-checks\n",
    "# =========================\n",
    "\n",
    "def validate_rubric(rubric: Dict[str, Any]) -> List[str]:\n",
    "    errors = []\n",
    "    for err in sorted(RUBRIC_VALIDATOR.iter_errors(rubric), key=lambda e: e.path):\n",
    "        loc = \"/\".join([str(x) for x in err.path])\n",
    "        errors.append(f\"{loc}: {err.message}\")\n",
    "    # Additional semantic checks:\n",
    "    # 1) categorical: descriptor keys ‚äÜ levels\n",
    "    try:\n",
    "        if rubric[\"scale\"][\"type\"] == \"categorical\":\n",
    "            levels = set(rubric[\"scale\"].get(\"levels\") or [])\n",
    "            for i, c in enumerate(rubric[\"criteria\"]):\n",
    "                bad = [k for k in c[\"descriptor_by_level\"].keys() if k not in levels]\n",
    "                if bad:\n",
    "                    errors.append(f\"criteria[{i}].descriptor_by_level has keys not in scale.levels: {bad}\")\n",
    "    except KeyError:\n",
    "        pass\n",
    "    # 2) numeric: min < max (jsonschema also checks types)\n",
    "    if rubric[\"scale\"][\"type\"] == \"numeric\":\n",
    "        mn = rubric[\"scale\"].get(\"min\"); mx = rubric[\"scale\"].get(\"max\")\n",
    "        if isinstance(mn, (int,float)) and isinstance(mx, (int,float)) and not (mn < mx):\n",
    "            errors.append(\"scale.min must be < scale.max\")\n",
    "    # 3) unique criterion names (case-insensitive)\n",
    "    names = [c[\"name\"].strip().lower() for c in rubric.get(\"criteria\", [])]\n",
    "    if len(set(names)) != len(names):\n",
    "        errors.append(\"criteria names must be unique (case-insensitive)\")\n",
    "    return errors\n",
    "\n",
    "# =========================\n",
    "# Public entry point\n",
    "# =========================\n",
    "\n",
    "def parse_rubric_file(path: str, model: str = \"gpt-4.1-mini\") -> Dict[str, Any]:\n",
    "    raw_text, parse_hint_method = extract_text_from_file(path)\n",
    "    if not raw_text or len(raw_text.strip()) < 30:\n",
    "        raise ValueError(\"Could not extract enough text from the file for parsing.\")\n",
    "\n",
    "    rubric = parse_rubric_with_llm(raw_text, parse_hint_method, model=model)\n",
    "    problems = validate_rubric(rubric)\n",
    "    if problems:\n",
    "        # attach validator findings as warnings\n",
    "        rubric.setdefault(\"source_parse\", {}).setdefault(\"warnings\", [])\n",
    "        rubric[\"source_parse\"][\"warnings\"].extend([f\"validation: {p}\" for p in problems])\n",
    "    return rubric\n",
    "\n",
    "# =========================\n",
    "# Demo and testing functions\n",
    "# =========================\n",
    "\n",
    "def demo_parse_rubric(file_path: str, model: str = \"gpt-4.1-mini\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Demo function to parse a rubric file and return the result.\n",
    "    Use this in Jupyter notebooks instead of command line execution.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = parse_rubric_file(file_path, model=model)\n",
    "        print(\"‚úÖ Rubric parsed successfully!\")\n",
    "        print(f\"üìä Found {len(result.get('criteria', []))} criteria\")\n",
    "        print(f\"üìè Scale type: {result.get('scale', {}).get('type', 'unknown')}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error parsing rubric: {e}\")\n",
    "        return {}\n",
    "\n",
    "def print_rubric_summary(rubric: Dict[str, Any]):\n",
    "    \"\"\"Print a formatted summary of the parsed rubric.\"\"\"\n",
    "    if not rubric:\n",
    "        print(\"No rubric data to display\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"üìã RUBRIC: {rubric.get('title', 'Untitled')}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Scale info\n",
    "    scale = rubric.get('scale', {})\n",
    "    print(f\"üìè Scale Type: {scale.get('type', 'unknown')}\")\n",
    "    if scale.get('type') == 'categorical':\n",
    "        levels = scale.get('levels', [])\n",
    "        print(f\"üìä Levels: {' ‚Üí '.join(levels) if levels else 'None'}\")\n",
    "    elif scale.get('type') == 'numeric':\n",
    "        min_val = scale.get('min')\n",
    "        max_val = scale.get('max')\n",
    "        print(f\"üìä Range: {min_val} - {max_val}\")\n",
    "    \n",
    "    # Criteria\n",
    "    criteria = rubric.get('criteria', [])\n",
    "    print(f\"\\nüìù Criteria ({len(criteria)}):\")\n",
    "    for i, criterion in enumerate(criteria, 1):\n",
    "        name = criterion.get('name', 'Unnamed')\n",
    "        weight = criterion.get('weight', 1.0)\n",
    "        print(f\"  {i}. {name} (weight: {weight})\")\n",
    "    \n",
    "    # Warnings\n",
    "    warnings = rubric.get('source_parse', {}).get('warnings', [])\n",
    "    if warnings:\n",
    "        print(f\"\\n‚ö†Ô∏è  Warnings ({len(warnings)}):\")\n",
    "        for warning in warnings:\n",
    "            print(f\"  ‚Ä¢ {warning}\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "# Example usage for Jupyter notebook:\n",
    "# result = demo_parse_rubric(\"path/to/your/rubric.pdf\")\n",
    "# print_rubric_summary(result)\n",
    "\n",
    "# CLI functionality (only runs when script is executed directly, not in notebook)\n",
    "if __name__ == \"__main__\" and not hasattr(__builtins__, '__IPYTHON__'):\n",
    "    import argparse, pprint\n",
    "    ap = argparse.ArgumentParser(description=\"Parse a rubric file into canonical JSON.\")\n",
    "    ap.add_argument(\"file\", help=\"Path to rubric: .txt .docx .pdf .png .jpg\")\n",
    "    ap.add_argument(\"--model\", default=\"gpt-4.1-mini\", help=\"OpenAI model (supports Structured Outputs).\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    result = parse_rubric_file(args.file, model=args.model)\n",
    "    print(json.dumps(result, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb720c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Error parsing rubric: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n",
      "No rubric data to display\n",
      "‚úÖ Rubric parser functions loaded successfully!\n",
      "üìù Available functions:\n",
      "  ‚Ä¢ demo_parse_rubric(file_path, model='gpt-4.1-mini') - Parse a rubric file\n",
      "  ‚Ä¢ print_rubric_summary(rubric) - Display formatted summary\n",
      "  ‚Ä¢ parse_rubric_file(file_path, model) - Core parsing function\n",
      "  ‚Ä¢ extract_text_from_file(file_path) - Extract text from various file formats\n",
      "\n",
      "üí° To use: Uncomment the example code above and provide a valid file path\n"
     ]
    }
   ],
   "source": [
    "# Example usage in Jupyter notebook\n",
    "# Uncomment and modify the path below to test with your rubric file\n",
    "\n",
    "# Example 1: Parse a rubric file\n",
    "file_path = \"test_file/test_rubric.docx\"  # or .docx, .txt, .png, .jpg\n",
    "result = demo_parse_rubric(file_path)\n",
    "print_rubric_summary(result)\n",
    "\n",
    "# Example 2: Just load the functions without running\n",
    "# print(\"‚úÖ Rubric parser functions loaded successfully!\")\n",
    "# print(\"üìù Available functions:\")\n",
    "# print(\"  ‚Ä¢ demo_parse_rubric(file_path, model='gpt-4.1-mini') - Parse a rubric file\")\n",
    "# print(\"  ‚Ä¢ print_rubric_summary(rubric) - Display formatted summary\")\n",
    "# print(\"  ‚Ä¢ parse_rubric_file(file_path, model) - Core parsing function\")\n",
    "# print(\"  ‚Ä¢ extract_text_from_file(file_path) - Extract text from various file formats\")\n",
    "# print(\"\\nüí° To use: Uncomment the example code above and provide a valid file path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94445cd3-27c5-4382-87a2-e9b256eadc88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
