{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4936ef4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… API key is set and ready to use!\n",
      "ðŸ”‘ Key starts with: sk-proj-...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import math\n",
    "import statistics\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, List, Any, Optional, Tuple, Literal\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# --------- OpenAI client ----------\n",
    "# pip install openai>=1.0.0\n",
    "from openai import OpenAI\n",
    "OPENAI_MODEL = os.environ.get(\"RUBRICHECK_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "# Read API key from api.txt file\n",
    "def get_api_key_from_file(file_path: str = r\"C:\\Users\\Leo\\AI projects\\_api.txt\", keyname: str = \"RubricParserPrompt\") -> str:\n",
    "    \"\"\"Read API key from api.txt file for rubriCheck project.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip().startswith(f'{keyname}:'):\n",
    "                    return line.strip().split(':', 1)[1].strip()\n",
    "        raise ValueError(\"rubriCheck API key not found in file\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"API file not found at {file_path}\")\n",
    "\n",
    "# Set the API key from file\n",
    "api_file = r\"C:\\Users\\Leo\\AI projects\\_api.txt\"\n",
    "keyname = \"RubricParserPrompt\"\n",
    "api_key = get_api_key_from_file()\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Verify the API key is set\n",
    "try:\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    if api_key == \"your-api-key-here\":\n",
    "        print(\"âš ï¸  Please replace 'your-api-key-here' with your actual OpenAI API key!\")\n",
    "    else:\n",
    "        print(\"âœ… API key is set and ready to use!\")\n",
    "        print(f\"ðŸ”‘ Key starts with: {api_key[:8]}...\")\n",
    "except KeyError:\n",
    "    print(\"âŒ API key not found in environment variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4611b147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ===============================\n",
    "# Data structures (expected inputs)\n",
    "# ===============================\n",
    "\n",
    "# Expected rubric structure (already parsed/normalized by your parser):\n",
    "# {\n",
    "#   \"criteria\": [\n",
    "#     {\n",
    "#       \"id\": \"thesis\",\n",
    "#       \"name\": \"Thesis & Focus\",\n",
    "#       \"descriptors\": {\n",
    "#         \"Excellent\": \"Clear, arguable thesis driving the essay...\",\n",
    "#         \"Good\": \"Thesis present but may be somewhat broad...\",\n",
    "#         \"Fair\": \"Thesis is unclear or only implied...\",\n",
    "#         \"Poor\": \"No discernible thesis...\"\n",
    "#       },\n",
    "#       \"valid_levels\": [\"Excellent\",\"Good\",\"Fair\",\"Poor\"],\n",
    "#       \"weight\": 0.25,  # optional; default = equal weights if missing\n",
    "#       \"level_scale_note\": \"Excellent > Good > Fair > Poor\"\n",
    "#     },\n",
    "#     ...\n",
    "#   ],\n",
    "#   \"grading\": {\n",
    "#       \"numeric\": True,                  # if True, weights are numeric; compute weighted avg\n",
    "#       \"letter_bands\": [                 # optional; maps numeric to letters\n",
    "#          {\"min\": 90, \"max\": 100, \"letter\": \"A+\"},\n",
    "#          {\"min\": 85, \"max\": 89.99, \"letter\": \"A\"},\n",
    "#          ...\n",
    "#       ],\n",
    "#       # If numeric=False or missing, we treat as categorical-only and map level->points:\n",
    "#       \"categorical_points_map\": {\"Excellent\": 4, \"Good\": 3, \"Fair\": 2, \"Poor\": 1}\n",
    "#   }\n",
    "# }\n",
    "\n",
    "# Essay paragraphs (privacy-processed, chunked upstream if needed):\n",
    "# essay_paragraphs = [\"Para 0 text...\", \"Para 1 text...\", ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f70e3d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# Prompt templates\n",
    "# ===============================\n",
    "\n",
    "SYSTEM_BASE = \"\"\"You are RubriCheck, an AI grader that outputs strict JSON for each criterion. \n",
    "Rules:\n",
    "- Only quote text present in the essay. Do not fabricate citations.\n",
    "- For evidence, quote short excerpts (<= {max_span_chars} characters per excerpt) and include paragraph indices.\n",
    "- If the rubric content for this criterion is ambiguous or self-contradictory, REFUSE with {{\"refuse\": true, \"reason\": \"...\"}} using the exact JSON schema.\n",
    "- No praise; provide one actionable suggestion per criterion.\n",
    "- Output MUST be valid JSON with the exact keys specifiedâ€”no extra keys, no prose outside JSON.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7ff7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Strict JSON schema instructions (enforced by instruction + examples).\n",
    "# We keep keys stable and visible in the user prompt.\n",
    "CRITERION_OUTPUT_KEYS = [\n",
    "    \"criterion_id\", \"valid_levels\", \"level\", \"justification\", \"evidence_spans\",\n",
    "    \"actionable_suggestion\", \"refuse\", \"reason\"\n",
    "]\n",
    "\n",
    "def make_criterion_user_prompt(\n",
    "    criterion: Dict[str, Any],\n",
    "    essay_paragraphs: List[str],\n",
    "    max_span_chars: int = 240\n",
    ") -> str:\n",
    "    # Only pass relevant chunk: here we pass full paragraphs; upstream you may pass\n",
    "    # paragraph indices likely relevant to this criterion. Keep it simple & auditable.\n",
    "    essay_block = \"\\n\".join([f\"[{i}] {p}\" for i, p in enumerate(essay_paragraphs)])\n",
    "    valid_levels_list = criterion[\"valid_levels\"]\n",
    "    descriptors = criterion[\"descriptors\"]\n",
    "\n",
    "    return f\"\"\"\n",
    "You will grade ONE criterion only.\n",
    "\n",
    "CRITERION\n",
    "- criterion_id: {criterion.get('id')}\n",
    "- name: {criterion.get('name')}\n",
    "- valid_levels (choose EXACTLY one): {valid_levels_list}\n",
    "- level scale note: {criterion.get('level_scale_note', '')}\n",
    "\n",
    "DESCRIPTORS (for this criterion only)\n",
    "{descriptors}\n",
    "\n",
    "ESSAY (paragraph-indexed)\n",
    "{essay_block}\n",
    "\n",
    "REQUIREMENTS\n",
    "1) Return STRICT JSON with EXACTLY these keys (no others):\n",
    "{CRITERION_OUTPUT_KEYS}\n",
    "2) \"valid_levels\": repeat a short list (the same list above) for transparency.\n",
    "3) \"level\": MUST be one of valid_levels. If unsure/ambiguous, set \"refuse\"=true and fill \"reason\".\n",
    "4) \"justification\": 1â€“3 sentences explaining why chosen level matches the descriptor.\n",
    "5) \"evidence_spans\": array of objects, each with:\n",
    "   - \"paragraph_index\" (integer)\n",
    "   - \"quote\" (string, <= {max_span_chars} chars, must appear verbatim in that paragraph)\n",
    "6) \"actionable_suggestion\": one concrete, specific improvement step for this criterion.\n",
    "7) Safety: Never invent content; only quote from the essay paragraphs provided.\n",
    "\n",
    "Return ONLY the JSON objectâ€”no commentary.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8020e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second pass self-check prompt (reads modelâ€™s chosen level & justification)\n",
    "def make_consistency_prompt(chosen: Dict[str, Any], criterion: Dict[str, Any]) -> str:\n",
    "    return f\"\"\"\n",
    "You are verifying rubric consistency for ONE criterion.\n",
    "\n",
    "CRITERION DESCRIPTORS\n",
    "{criterion[\"descriptors\"]}\n",
    "\n",
    "MODEL CHOICE\n",
    "- level: {chosen.get(\"level\")}\n",
    "- justification: {chosen.get(\"justification\")}\n",
    "- evidence_spans: {chosen.get(\"evidence_spans\")}\n",
    "\n",
    "TASK\n",
    "Explain why (or why not) the chosen level matches the descriptor language. \n",
    "If you find contradiction or weak alignment, say \"low_confidence\": true and explain briefly; else \"low_confidence\": false. \n",
    "Output JSON ONLY with keys: [\"low_confidence\", \"explanation\"].\n",
    "\"\"\"\n",
    "\n",
    "# Slightly perturbed prompts for agreement check\n",
    "def make_agreement_variant_prompt(base_prompt: str, variant_tag: str) -> str:\n",
    "    # Minimal perturbations: reorder rules, add harmless synonym, etc.\n",
    "    return base_prompt + f\"\\n# variant_tag: {variant_tag}\\n\" \\\n",
    "                         f\"Note: Verify factual quotes strictly; do not exceed span length.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa005543",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ===============================\n",
    "# Core LLM helpers\n",
    "# ===============================\n",
    "\n",
    "def llm_json(prompt: str, system: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calls the OpenAI chat completion and attempts to parse JSON only.\n",
    "    We ask the model to output ONLY JSON. If it fails, we rethrow with the raw text.\n",
    "    \"\"\"\n",
    "    resp = client.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        temperature=0.2,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    txt = resp.choices[0].message.content.strip()\n",
    "    # Simple JSON guard:\n",
    "    # Extract first JSON object found:\n",
    "    # Find the first { and try to parse from there\n",
    "    start_idx = txt.find('{')\n",
    "    if start_idx == -1:\n",
    "        raise ValueError(f\"Expected JSON but got:\\n{txt}\")\n",
    "    \n",
    "    # Try to parse JSON starting from the first {\n",
    "    try:\n",
    "        return json.loads(txt[start_idx:])\n",
    "    except json.JSONDecodeError:\n",
    "        # If that fails, try to find the end of the JSON object\n",
    "        brace_count = 0\n",
    "        end_idx = start_idx\n",
    "        for i, char in enumerate(txt[start_idx:], start_idx):\n",
    "            if char == '{':\n",
    "                brace_count += 1\n",
    "            elif char == '}':\n",
    "                brace_count -= 1\n",
    "                if brace_count == 0:\n",
    "                    end_idx = i + 1\n",
    "                    break\n",
    "        \n",
    "        try:\n",
    "            return json.loads(txt[start_idx:end_idx])\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(f\"Invalid JSON returned:\\n{txt}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce8f1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ===============================\n",
    "# Scoring + Aggregation\n",
    "# ===============================\n",
    "\n",
    "@dataclass\n",
    "class CriterionResult:\n",
    "    criterion_id: str\n",
    "    valid_levels: List[str]\n",
    "    level: Optional[str]\n",
    "    justification: Optional[str]\n",
    "    evidence_spans: List[Dict[str, Any]]\n",
    "    actionable_suggestion: Optional[str]\n",
    "    refuse: bool\n",
    "    reason: Optional[str]\n",
    "    low_confidence: bool = False\n",
    "    consistency_explanation: Optional[str] = None\n",
    "    agreement_flag: Literal[\"ok\",\"needs_review\",\"tie_break\"] = \"ok\"\n",
    "    tie_break_used: bool = False\n",
    "\n",
    "@dataclass\n",
    "class GradeSummary:\n",
    "    per_criterion: List[CriterionResult] = field(default_factory=list)\n",
    "    numeric_score: Optional[float] = None\n",
    "    letter: Optional[str] = None\n",
    "    categorical_points: Optional[float] = None\n",
    "    notes: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "def tie_break_choice(a: str, b: str, valid_levels: List[str]) -> str:\n",
    "    # Prefer the higher of the two by scale order (left->right descending quality).\n",
    "    # Assuming valid_levels is ordered from best to worst (as provided).\n",
    "    # If order unknown, we keep 'a' by default.\n",
    "    try:\n",
    "        ia = valid_levels.index(a)\n",
    "        ib = valid_levels.index(b)\n",
    "        return a if ia < ib else b\n",
    "    except:\n",
    "        return a\n",
    "\n",
    "def map_levels_to_points(levels: Dict[str, int], chosen_level: str) -> Optional[int]:\n",
    "    return levels.get(chosen_level)\n",
    "\n",
    "def compute_weighted_numeric(per: List[CriterionResult], rubric: Dict[str, Any]) -> Tuple[Optional[float], Optional[str]]:\n",
    "    # Weighted mean of mapped numeric points if 'numeric' grading is configured.\n",
    "    if not rubric.get(\"grading\", {}).get(\"numeric\", False):\n",
    "        return None, None\n",
    "\n",
    "    # If descriptors embed numeric thresholds, youâ€™d parse them upstream.\n",
    "    # Here, we map text levels onto provisional numeric anchors (Excellent=100, Good=85, Fair=70, Poor=55) as a fallback.\n",
    "    fallback_map = {\"Excellent\": 100, \"Good\": 85, \"Fair\": 70, \"Poor\": 55}\n",
    "    crits = rubric[\"criteria\"]\n",
    "    weights = []\n",
    "    scores = []\n",
    "    for c in per:\n",
    "        crit_meta = next((x for x in crits if x[\"id\"] == c.criterion_id), None)\n",
    "        w = crit_meta.get(\"weight\", None) if crit_meta else None\n",
    "        if w is None:\n",
    "            # equal weights later if any are missing\n",
    "            pass\n",
    "        weights.append(w)\n",
    "\n",
    "    # Normalize weights: if any missing, treat all as equal\n",
    "    if any(w is None for w in weights):\n",
    "        weights = [1.0 for _ in per]\n",
    "    total_w = sum(weights) or 1.0\n",
    "\n",
    "    numeric_scores = []\n",
    "    for c, w in zip(per, weights):\n",
    "        if c.level is None:\n",
    "            continue\n",
    "        s = fallback_map.get(c.level)\n",
    "        if s is not None:\n",
    "            numeric_scores.append((s, w))\n",
    "\n",
    "    if not numeric_scores:\n",
    "        return None, None\n",
    "\n",
    "    weighted = sum(s * w for s, w in numeric_scores) / total_w\n",
    "    # Letter band mapping if provided\n",
    "    letter = None\n",
    "    bands = rubric.get(\"grading\", {}).get(\"letter_bands\", [])\n",
    "    for band in bands:\n",
    "        if band[\"min\"] <= weighted <= band[\"max\"]:\n",
    "            letter = band[\"letter\"]\n",
    "            break\n",
    "    return weighted, letter\n",
    "\n",
    "def compute_categorical_aggregate(per: List[CriterionResult], rubric: Dict[str, Any]) -> Optional[float]:\n",
    "    # Map levels->points then average (unweighted) unless you want weighted here as well.\n",
    "    cmap = rubric.get(\"grading\", {}).get(\"categorical_points_map\", {\"Excellent\":4,\"Good\":3,\"Fair\":2,\"Poor\":1})\n",
    "    vals = []\n",
    "    for c in per:\n",
    "        if c.level:\n",
    "            pt = cmap.get(c.level)\n",
    "            if pt is not None:\n",
    "                vals.append(pt)\n",
    "    if not vals:\n",
    "        return None\n",
    "    return statistics.mean(vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceaa9b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ===============================\n",
    "# Main per-criterion evaluation flow\n",
    "# ===============================\n",
    "\n",
    "def evaluate_one_criterion(\n",
    "    criterion: Dict[str, Any],\n",
    "    essay_paragraphs: List[str],\n",
    "    max_span_chars: int = 240\n",
    ") -> CriterionResult:\n",
    "    system = SYSTEM_BASE.format(max_span_chars=max_span_chars)\n",
    "    base_prompt = make_criterion_user_prompt(criterion, essay_paragraphs, max_span_chars)\n",
    "\n",
    "    # Agreement check: run two slightly perturbed versions\n",
    "    out1 = llm_json(make_agreement_variant_prompt(base_prompt, \"A\"), system)\n",
    "    out2 = llm_json(make_agreement_variant_prompt(base_prompt, \"B\"), system)\n",
    "\n",
    "    # If either refused, mark refuse (you can decide a stricter policy)\n",
    "    refused = (out1.get(\"refuse\") is True) or (out2.get(\"refuse\") is True)\n",
    "\n",
    "    # Choose final level:\n",
    "    lvl1 = out1.get(\"level\")\n",
    "    lvl2 = out2.get(\"level\")\n",
    "    final = out1\n",
    "    agreement_flag = \"ok\"\n",
    "    tie_break_used = False\n",
    "\n",
    "    if not refused and lvl1 and lvl2 and lvl1 != lvl2:\n",
    "        agreement_flag = \"needs_review\"\n",
    "        # optional tie-break third pass:\n",
    "        out3 = llm_json(make_agreement_variant_prompt(base_prompt, \"TIE_BREAK\"), system)\n",
    "        lvl3 = out3.get(\"level\")\n",
    "        if lvl3 and (lvl3 == lvl1 or lvl3 == lvl2):\n",
    "            final = out3\n",
    "            agreement_flag = \"ok\"\n",
    "            tie_break_used = True\n",
    "        else:\n",
    "            # If still no agreement, choose by scale order as a pragmatic default,\n",
    "            # and keep the \"needs_review\" flag.\n",
    "            final_level = tie_break_choice(lvl1, lvl2, criterion[\"valid_levels\"])\n",
    "            final = {**out1, \"level\": final_level}\n",
    "\n",
    "    # Consistency self-check\n",
    "    consistency = llm_json(\n",
    "        make_consistency_prompt(final, criterion),\n",
    "        system=\"You are a strict JSON validator.\"\n",
    "    )\n",
    "    low_conf = bool(consistency.get(\"low_confidence\"))\n",
    "    explanation = consistency.get(\"explanation\")\n",
    "\n",
    "    return CriterionResult(\n",
    "        criterion_id=final.get(\"criterion_id\", criterion.get(\"id\")),\n",
    "        valid_levels=final.get(\"valid_levels\", criterion[\"valid_levels\"]),\n",
    "        level=final.get(\"level\"),\n",
    "        justification=final.get(\"justification\"),\n",
    "        evidence_spans=final.get(\"evidence_spans\", []),\n",
    "        actionable_suggestion=final.get(\"actionable_suggestion\"),\n",
    "        refuse=bool(final.get(\"refuse\", False) or refused),\n",
    "        reason=final.get(\"reason\"),\n",
    "        low_confidence=low_conf,\n",
    "        consistency_explanation=explanation,\n",
    "        agreement_flag=\"ok\" if not refused and not low_conf and agreement_flag==\"ok\" else agreement_flag,\n",
    "        tie_break_used=tie_break_used\n",
    "    )\n",
    "\n",
    "def grade_essay(\n",
    "    rubric: Dict[str, Any],\n",
    "    essay_paragraphs: List[str],\n",
    "    max_span_chars: int = 240\n",
    ") -> GradeSummary:\n",
    "    results: List[CriterionResult] = []\n",
    "    for crit in rubric[\"criteria\"]:\n",
    "        res = evaluate_one_criterion(crit, essay_paragraphs, max_span_chars=max_span_chars)\n",
    "        results.append(res)\n",
    "\n",
    "    # Aggregations\n",
    "    numeric_score, letter = compute_weighted_numeric(results, rubric)\n",
    "    categorical_points = None\n",
    "    if not rubric.get(\"grading\", {}).get(\"numeric\", False):\n",
    "        categorical_points = compute_categorical_aggregate(results, rubric)\n",
    "        # if you ALSO want to show numeric (mapped) alongside categorical, you can compute both.\n",
    "\n",
    "    # Notes about reliability/safety\n",
    "    flags = {\n",
    "        \"any_refusals\": any(r.refuse for r in results),\n",
    "        \"any_low_confidence\": any(r.low_confidence for r in results),\n",
    "        \"any_needs_review\": any(r.agreement_flag != \"ok\" for r in results),\n",
    "    }\n",
    "\n",
    "    return GradeSummary(\n",
    "        per_criterion=results,\n",
    "        numeric_score=None if numeric_score is None else round(numeric_score, 2),\n",
    "        letter=letter,\n",
    "        categorical_points=None if categorical_points is None else round(categorical_points, 2),\n",
    "        notes=flags\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b29c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"per_criterion\": [\n",
      "    {\n",
      "      \"criterion_id\": \"thesis\",\n",
      "      \"valid_levels\": [\n",
      "        \"Excellent\",\n",
      "        \"Good\",\n",
      "        \"Fair\",\n",
      "        \"Poor\"\n",
      "      ],\n",
      "      \"level\": \"Good\",\n",
      "      \"justification\": \"The thesis is present and argues for the importance of renewable energy to national security, but it could be more focused and specific.\",\n",
      "      \"evidence_spans\": [\n",
      "        {\n",
      "          \"paragraph_index\": 0,\n",
      "          \"quote\": \"This essay argues that renewable energy is essential to national security by reducing dependence on volatile fuel markets.\"\n",
      "        }\n",
      "      ],\n",
      "      \"actionable_suggestion\": \"Refine the thesis to specify how renewable energy contributes to national security beyond just reducing dependence on fuel markets.\",\n",
      "      \"refuse\": false,\n",
      "      \"reason\": \"\",\n",
      "      \"low_confidence\": false,\n",
      "      \"consistency_explanation\": \"The chosen level 'Good' aligns well with the descriptor language. The justification states that the thesis is present and argues for the importance of renewable energy to national security, which matches the descriptor's indication that the thesis is somewhat broad or unevenly maintained. The evidence span supports this by providing a clear thesis statement, although it suggests that the focus could be improved, consistent with the 'Good' level description.\",\n",
      "      \"agreement_flag\": \"ok\",\n",
      "      \"tie_break_used\": false\n",
      "    },\n",
      "    {\n",
      "      \"criterion_id\": \"evidence\",\n",
      "      \"valid_levels\": [\n",
      "        \"Excellent\",\n",
      "        \"Good\",\n",
      "        \"Fair\",\n",
      "        \"Poor\"\n",
      "      ],\n",
      "      \"level\": \"Good\",\n",
      "      \"justification\": \"The essay provides generally apt evidence regarding renewable energy's impact on national security, though some analysis is needed for clarity.\",\n",
      "      \"evidence_spans\": [\n",
      "        {\n",
      "          \"paragraph_index\": 1,\n",
      "          \"quote\": \"Several reports show countries with higher renewable portfolios experience less price shock; however, grid stability challenges remain.\"\n",
      "        },\n",
      "        {\n",
      "          \"paragraph_index\": 2,\n",
      "          \"quote\": \"this essay demonstrates recent cost curves and policy mechanisms that offset initial investment.\"\n",
      "        }\n",
      "      ],\n",
      "      \"actionable_suggestion\": \"Enhance the analysis of the evidence provided to clarify how it supports the argument.\",\n",
      "      \"refuse\": false,\n",
      "      \"reason\": \"\",\n",
      "      \"low_confidence\": false,\n",
      "      \"consistency_explanation\": \"The chosen level 'Good' aligns well with the descriptor language. The justification notes that the evidence provided is generally apt, which corresponds to the descriptor stating 'Evidence is generally apt; some analysis; minor lapses.' The evidence spans also reflect relevant points about renewable energy's impact, supporting the analysis while acknowledging that further clarity is needed, which is consistent with the 'Good' level.\",\n",
      "      \"agreement_flag\": \"ok\",\n",
      "      \"tie_break_used\": false\n",
      "    }\n",
      "  ],\n",
      "  \"numeric_score\": 85.0,\n",
      "  \"letter\": \"A\",\n",
      "  \"categorical_points\": null,\n",
      "  \"notes\": {\n",
      "    \"any_refusals\": false,\n",
      "    \"any_low_confidence\": false,\n",
      "    \"any_needs_review\": false\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# Example usage\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    import json\n",
    "\n",
    "    # Example rubric (minimal):\n",
    "    rubric = {\n",
    "        \"criteria\": [\n",
    "            {\n",
    "                \"id\": \"thesis\",\n",
    "                \"name\": \"Thesis & Focus\",\n",
    "                \"descriptors\": {\n",
    "                    \"Excellent\": \"Clear, arguable thesis driving the essay.\",\n",
    "                    \"Good\": \"Thesis is present but somewhat broad or unevenly maintained.\",\n",
    "                    \"Fair\": \"Thesis is unclear, implied, or inconsistently applied.\",\n",
    "                    \"Poor\": \"No discernible thesis or focus.\"\n",
    "                },\n",
    "                \"valid_levels\": [\"Excellent\",\"Good\",\"Fair\",\"Poor\"],\n",
    "                \"weight\": 0.25,\n",
    "                \"level_scale_note\": \"Excellent > Good > Fair > Poor\"\n",
    "            },\n",
    "            {\n",
    "                \"id\": \"evidence\",\n",
    "                \"name\": \"Use of Evidence\",\n",
    "                \"descriptors\": {\n",
    "                    \"Excellent\": \"Integrates specific, well-chosen evidence; accurately cited; analysis is insightful.\",\n",
    "                    \"Good\": \"Evidence is generally apt; some analysis; minor lapses.\",\n",
    "                    \"Fair\": \"Evidence is limited, vague, or inconsistently analyzed.\",\n",
    "                    \"Poor\": \"Little to no relevant evidence; analysis missing.\"\n",
    "                },\n",
    "                \"valid_levels\": [\"Excellent\",\"Good\",\"Fair\",\"Poor\"],\n",
    "                \"weight\": 0.25,\n",
    "                \"level_scale_note\": \"Excellent > Good > Fair > Poor\"\n",
    "            }\n",
    "        ],\n",
    "        \"grading\": {\n",
    "            \"numeric\": True,\n",
    "            \"letter_bands\": [\n",
    "                {\"min\": 90, \"max\": 100, \"letter\": \"A+\"},\n",
    "                {\"min\": 85, \"max\": 89.99, \"letter\": \"A\"},\n",
    "                {\"min\": 80, \"max\": 84.99, \"letter\": \"A-\"},\n",
    "                {\"min\": 0, \"max\": 79.99, \"letter\": \"B or below\"}\n",
    "            ],\n",
    "            \"categorical_points_map\": {\"Excellent\": 4, \"Good\": 3, \"Fair\": 2, \"Poor\": 1}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    essay_paragraphs = [\n",
    "        \"This essay argues that renewable energy is essential to national security by reducing dependence on volatile fuel markets.\",\n",
    "        \"Several reports show countries with higher renewable portfolios experience less price shock; however, grid stability challenges remain.\",\n",
    "        \"Opponents claim costs are prohibitive; this essay demonstrates recent cost curves and policy mechanisms that offset initial investment.\",\n",
    "    ]\n",
    "\n",
    "    summary = grade_essay(rubric, essay_paragraphs, max_span_chars=180)\n",
    "    print(json.dumps({\n",
    "        \"per_criterion\": [r.__dict__ for r in summary.per_criterion],\n",
    "        \"numeric_score\": summary.numeric_score,\n",
    "        \"letter\": summary.letter,\n",
    "        \"categorical_points\": summary.categorical_points,\n",
    "        \"notes\": summary.notes\n",
    "    }, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69465131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
