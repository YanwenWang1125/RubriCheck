{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef600161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API key is set and ready to use!\n",
      "🔑 Key starts with: sk-proj-...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, io, re, json, tempfile, mimetypes, math\n",
    "from typing import Tuple, Dict, Any, Optional, List\n",
    "\n",
    "# --- File extraction deps\n",
    "import fitz  # PyMuPDF\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import docx2txt\n",
    "\n",
    "# --- OpenAI (Responses API with Structured Outputs)\n",
    "from openai import OpenAI\n",
    "\n",
    "# --- Validation\n",
    "from jsonschema import Draft7Validator\n",
    "\n",
    "# Read API key from api.txt file\n",
    "def get_api_key_from_file(file_path: str = r\"C:\\Users\\Leo\\AI projects\\_api.txt\", keyname: str = \"RubricParserPrompt\") -> str:\n",
    "    \"\"\"Read API key from api.txt file for rubriCheck project.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.strip().startswith(f'{keyname}:'):\n",
    "                    return line.strip().split(':', 1)[1].strip()\n",
    "        raise ValueError(\"rubriCheck API key not found in file\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"API file not found at {file_path}\")\n",
    "\n",
    "# Set the API key from file\n",
    "api_file = r\"C:\\Users\\Leo\\AI projects\\_api.txt\"\n",
    "keyname = \"RubricParserPrompt\"\n",
    "api_key = get_api_key_from_file()\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
    "\n",
    "# Verify the API key is set\n",
    "try:\n",
    "    api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "    if api_key == \"your-api-key-here\":\n",
    "        print(\"⚠️  Please replace 'your-api-key-here' with your actual OpenAI API key!\")\n",
    "    else:\n",
    "        print(\"✅ API key is set and ready to use!\")\n",
    "        print(f\"🔑 Key starts with: {api_key[:8]}...\")\n",
    "except KeyError:\n",
    "    print(\"❌ API key not found in environment variables\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a36d168e-7208-4352-80b9-02db79542316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rubric_parser.py\n",
    "# --------------------------------------------------------------------\n",
    "# Ingest TXT/DOCX/PDF/Image → extract text → LLM parse (Structured Outputs)\n",
    "# → local JSON Schema validation → return normalized rubric JSON\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# =========================\n",
    "# JSON SCHEMA (Structured)\n",
    "# =========================\n",
    "\n",
    "RUBRIC_JSON_SCHEMA: Dict[str, Any] = {\n",
    "    \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "    \"title\": \"RubricSchema\",\n",
    "    \"type\": \"object\",\n",
    "    \"additionalProperties\": False,\n",
    "    \"properties\": {\n",
    "        \"title\": {\"type\": [\"string\", \"null\"], \"maxLength\": 200},\n",
    "        \"scale\": {\n",
    "            \"type\": \"object\",\n",
    "            \"additionalProperties\": False,\n",
    "            \"properties\": {\n",
    "                \"type\": {\"type\": \"string\", \"enum\": [\"categorical\", \"numeric\"]},\n",
    "                \"levels\": {\n",
    "                    \"type\": [\"array\", \"null\"],\n",
    "                    \"items\": {\"type\": \"string\", \"minLength\": 1},\n",
    "                    \"minItems\": 1\n",
    "                },\n",
    "                \"min\": {\"type\": [\"number\", \"null\"]},\n",
    "                \"max\": {\"type\": [\"number\", \"null\"]},\n",
    "                \"original_levels\": {\n",
    "                    \"type\": [\"array\", \"null\"],\n",
    "                    \"items\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"synonyms\": {\n",
    "                    \"type\": [\"object\", \"null\"],\n",
    "                    \"additionalProperties\": {\"type\": \"string\"}\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"type\"]\n",
    "        },\n",
    "        \"criteria\": {\n",
    "            \"type\": \"array\",\n",
    "            \"minItems\": 1,\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"additionalProperties\": False,\n",
    "                \"properties\": {\n",
    "                    \"name\": {\"type\": \"string\", \"minLength\": 1, \"maxLength\": 120},\n",
    "                    \"descriptor_by_level\": {\n",
    "                        \"type\": \"object\",\n",
    "                        \"additionalProperties\": {\"type\": \"string\"}\n",
    "                    },\n",
    "                    \"weight\": {\"type\": \"number\", \"exclusiveMinimum\": 0},\n",
    "                    \"evidence_hint\": {\"type\": [\"string\", \"null\"]},\n",
    "                    \"notes\": {\"type\": [\"string\", \"null\"]}\n",
    "                },\n",
    "                \"required\": [\"name\", \"descriptor_by_level\"]\n",
    "            }\n",
    "        },\n",
    "        \"notes\": {\"type\": [\"string\", \"null\"]},\n",
    "        \"source_parse\": {\n",
    "            \"type\": \"object\",\n",
    "            \"additionalProperties\": False,\n",
    "            \"properties\": {\n",
    "                \"method\": {\"type\": \"string\", \"enum\": [\"table\", \"narrative\", \"hybrid\", \"ocr\"]},\n",
    "                \"confidence\": {\"type\": \"number\", \"minimum\": 0.0, \"maximum\": 1.0},\n",
    "                \"warnings\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"},\n",
    "                    \"default\": []\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"method\", \"confidence\"]\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"scale\", \"criteria\", \"source_parse\"]\n",
    "}\n",
    "\n",
    "# Pre-compile validator for speed\n",
    "RUBRIC_VALIDATOR = Draft7Validator(RUBRIC_JSON_SCHEMA)\n",
    "\n",
    "# =========================\n",
    "# File → text extraction\n",
    "# =========================\n",
    "\n",
    "IMG_EXT = {\".png\", \".jpg\", \".jpeg\", \".webp\", \".tif\", \".tiff\", \".bmp\"}\n",
    "\n",
    "def _deskew_and_binarize(pil_img: Image.Image) -> Image.Image:\n",
    "    \"\"\"Basic deskew + binarization to improve OCR.\"\"\"\n",
    "    img = np.array(pil_img.convert(\"L\"))  # grayscale\n",
    "    # threshold\n",
    "    _, th = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    # deskew\n",
    "    coords = np.column_stack(np.where(th == 0))\n",
    "    angle = 0.0\n",
    "    if coords.size > 0:\n",
    "        rect = cv2.minAreaRect(coords)\n",
    "        angle = rect[-1]\n",
    "        if angle < -45:\n",
    "            angle = -(90 + angle)\n",
    "        else:\n",
    "            angle = -angle\n",
    "    (h, w) = th.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1.0)\n",
    "    rotated = cv2.warpAffine(th, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)\n",
    "    return Image.fromarray(rotated)\n",
    "\n",
    "def _ocr_pil_image(pil_img: Image.Image, lang: str = \"eng\") -> str:\n",
    "    proc = _deskew_and_binarize(pil_img)\n",
    "    return pytesseract.image_to_string(proc, lang=lang)\n",
    "\n",
    "def _extract_from_pdf(path: str) -> Tuple[str, str]:\n",
    "    \"\"\"Return (text, method). Try native text first; fallback to OCR if text looks empty.\"\"\"\n",
    "    doc = fitz.open(path)\n",
    "    texts = []\n",
    "    for p in doc:\n",
    "        txt = p.get_text(\"text\")\n",
    "        if txt:\n",
    "            texts.append(txt)\n",
    "    native_text = \"\\n\".join(texts).strip()\n",
    "    if len(native_text) >= 400 or (len(native_text) > 40 and len(texts) >= 1):\n",
    "        return native_text, \"table\" if \" | \" in native_text or re.search(r\"\\bPoints?\\b\", native_text, re.I) else \"narrative\"\n",
    "\n",
    "    # Fallback to OCR\n",
    "    pages = convert_from_path(path, dpi=300)\n",
    "    ocr_texts = []\n",
    "    for pil in pages:\n",
    "        ocr_texts.append(_ocr_pil_image(pil))\n",
    "    return \"\\n\".join(ocr_texts).strip(), \"ocr\"\n",
    "\n",
    "def _extract_from_image(path: str) -> Tuple[str, str]:\n",
    "    pil = Image.open(path)\n",
    "    return _ocr_pil_image(pil), \"ocr\"\n",
    "\n",
    "def _extract_from_docx(path: str) -> Tuple[str, str]:\n",
    "    return docx2txt.process(path) or \"\", \"narrative\"\n",
    "\n",
    "def _extract_from_txt(path: str) -> Tuple[str, str]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read(), \"narrative\"\n",
    "\n",
    "def extract_text_from_file(path: str) -> Tuple[str, str]:\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "    if ext == \".pdf\":\n",
    "        return _extract_from_pdf(path)\n",
    "    if ext == \".docx\":\n",
    "        return _extract_from_docx(path)\n",
    "    if ext in IMG_EXT:\n",
    "        return _extract_from_image(path)\n",
    "    return _extract_from_txt(path)\n",
    "\n",
    "# =========================\n",
    "# OpenAI call (Structured Outputs)\n",
    "# =========================\n",
    "\n",
    "\n",
    "\n",
    "def _openai_client() -> OpenAI:\n",
    "    return OpenAI()  # reads OPENAI_API_KEY\n",
    "\n",
    "def parse_rubric_with_llm(raw_text: str, method_hint: str = \"narrative\", model: str = \"gpt-4o-mini\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calls the OpenAI Chat Completions API with JSON mode to get a strictly valid rubric JSON.\n",
    "    \"\"\"\n",
    "    client = _openai_client()\n",
    "\n",
    "    # Build the system message with schema instructions\n",
    "    system_message = (\n",
    "        \"You are a precise rubric parser. Convert the given rubric into strictly valid JSON that \"\n",
    "        \"conforms to the provided JSON Schema. Do not fabricate content. If information is missing, \"\n",
    "        \"omit it and write a warning. Preserve original level wording in descriptors; normalize level names.\\n\\n\"\n",
    "        \"IMPORTANT: You must respond with valid JSON only. No additional text or explanations.\"\n",
    "    )\n",
    "    \n",
    "    user_message = (\n",
    "        \"RAW_RUBRIC_TEXT:\\n```\\n\" + raw_text.strip() + \"\\n```\\n\\n\"\n",
    "        \"Instructions:\\n\"\n",
    "        f\"- The extraction method was '{method_hint}'. Set source_parse.method accordingly.\\n\"\n",
    "        \"- If levels like Excellent/Good/Fair/Poor are present, use them as categorical scale levels in best→worst order.\\n\"\n",
    "        \"- If a numeric points scale is present (e.g., 0–4), include scale.min/scale.max and set type='numeric'.\\n\"\n",
    "        \"- Parse weights when explicitly indicated (e.g., 'Clarity (30%)' → weight=0.30); otherwise default to 1.0.\\n\"\n",
    "        \"- If any descriptor is missing for a level, omit that key and add a warning.\\n\"\n",
    "        \"- If multiple rubrics are present, parse the first major rubric and add a warning.\\n\"\n",
    "        \"- Output only the JSON. No extra text.\\n\\n\"\n",
    "        \"Required JSON structure:\\n\"\n",
    "        \"{\\n\"\n",
    "        '  \"title\": \"string or null\",\\n'\n",
    "        '  \"scale\": {\\n'\n",
    "        '    \"type\": \"categorical or numeric\",\\n'\n",
    "        '    \"levels\": [\"array of strings for categorical\"],\\n'\n",
    "        '    \"min\": \"number for numeric\",\\n'\n",
    "        '    \"max\": \"number for numeric\"\\n'\n",
    "        '  },\\n'\n",
    "        '  \"criteria\": [\\n'\n",
    "        '    {\\n'\n",
    "        '      \"name\": \"string\",\\n'\n",
    "        '      \"descriptor_by_level\": {\"level\": \"description\"},\\n'\n",
    "        '      \"weight\": \"number\"\\n'\n",
    "        '    }\\n'\n",
    "        '  ],\\n'\n",
    "        '  \"source_parse\": {\\n'\n",
    "        '    \"method\": \"table/narrative/hybrid/ocr\",\\n'\n",
    "        '    \"confidence\": \"number 0.0-1.0\",\\n'\n",
    "        '    \"warnings\": [\"array of strings\"]\\n'\n",
    "        '  }\\n'\n",
    "        \"}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            response_format={\"type\": \"json_object\"},\n",
    "            max_tokens=2048,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        # Extract the JSON response\n",
    "        json_text = response.choices[0].message.content\n",
    "        parsed = json.loads(json_text)\n",
    "        \n",
    "        return parsed\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"❌ Failed to parse JSON response: {e}\")\n",
    "        print(f\"Raw response: {json_text}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"❌ OpenAI API error: {e}\")\n",
    "        raise\n",
    "\n",
    "# =========================\n",
    "# Validation & post-checks\n",
    "# =========================\n",
    "\n",
    "def validate_rubric(rubric: Dict[str, Any]) -> List[str]:\n",
    "    errors = []\n",
    "    for err in sorted(RUBRIC_VALIDATOR.iter_errors(rubric), key=lambda e: e.path):\n",
    "        loc = \"/\".join([str(x) for x in err.path])\n",
    "        errors.append(f\"{loc}: {err.message}\")\n",
    "    # Additional semantic checks:\n",
    "    # 1) categorical: descriptor keys ⊆ levels\n",
    "    try:\n",
    "        if rubric[\"scale\"][\"type\"] == \"categorical\":\n",
    "            levels = set(rubric[\"scale\"].get(\"levels\") or [])\n",
    "            for i, c in enumerate(rubric[\"criteria\"]):\n",
    "                bad = [k for k in c[\"descriptor_by_level\"].keys() if k not in levels]\n",
    "                if bad:\n",
    "                    errors.append(f\"criteria[{i}].descriptor_by_level has keys not in scale.levels: {bad}\")\n",
    "    except KeyError:\n",
    "        pass\n",
    "    # 2) numeric: min < max (jsonschema also checks types)\n",
    "    if rubric[\"scale\"][\"type\"] == \"numeric\":\n",
    "        mn = rubric[\"scale\"].get(\"min\"); mx = rubric[\"scale\"].get(\"max\")\n",
    "        if isinstance(mn, (int,float)) and isinstance(mx, (int,float)) and not (mn < mx):\n",
    "            errors.append(\"scale.min must be < scale.max\")\n",
    "    # 3) unique criterion names (case-insensitive)\n",
    "    names = [c[\"name\"].strip().lower() for c in rubric.get(\"criteria\", [])]\n",
    "    if len(set(names)) != len(names):\n",
    "        errors.append(\"criteria names must be unique (case-insensitive)\")\n",
    "    return errors\n",
    "\n",
    "# =========================\n",
    "# Public entry point\n",
    "# =========================\n",
    "\n",
    "def parse_rubric_file(path: str, model: str = \"gpt-4o-mini\") -> Dict[str, Any]:\n",
    "    raw_text, parse_hint_method = extract_text_from_file(path)\n",
    "    if not raw_text or len(raw_text.strip()) < 30:\n",
    "        raise ValueError(\"Could not extract enough text from the file for parsing.\")\n",
    "\n",
    "    rubric = parse_rubric_with_llm(raw_text, parse_hint_method, model=model)\n",
    "    problems = validate_rubric(rubric)\n",
    "    if problems:\n",
    "        # attach validator findings as warnings\n",
    "        rubric.setdefault(\"source_parse\", {}).setdefault(\"warnings\", [])\n",
    "        rubric[\"source_parse\"][\"warnings\"].extend([f\"validation: {p}\" for p in problems])\n",
    "    return rubric\n",
    "\n",
    "# =========================\n",
    "# Demo and testing functions\n",
    "# =========================\n",
    "\n",
    "def demo_parse_rubric(file_path: str, model: str = \"gpt-4o-mini\") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Demo function to parse a rubric file and return the result.\n",
    "    Use this in Jupyter notebooks instead of command line execution.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = parse_rubric_file(file_path, model=model)\n",
    "        print(\"✅ Rubric parsed successfully!\")\n",
    "        print(f\"📊 Found {len(result.get('criteria', []))} criteria\")\n",
    "        print(f\"📏 Scale type: {result.get('scale', {}).get('type', 'unknown')}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error parsing rubric: {e}\")\n",
    "        return {}\n",
    "\n",
    "def print_rubric_summary(rubric: Dict[str, Any]):\n",
    "    \"\"\"Print a formatted summary of the parsed rubric.\"\"\"\n",
    "    if not rubric:\n",
    "        print(\"No rubric data to display\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(f\"📋 RUBRIC: {rubric.get('title', 'Untitled')}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Scale info\n",
    "    scale = rubric.get('scale', {})\n",
    "    print(f\"📏 Scale Type: {scale.get('type', 'unknown')}\")\n",
    "    if scale.get('type') == 'categorical':\n",
    "        levels = scale.get('levels', [])\n",
    "        print(f\"📊 Levels: {' → '.join(levels) if levels else 'None'}\")\n",
    "    elif scale.get('type') == 'numeric':\n",
    "        min_val = scale.get('min')\n",
    "        max_val = scale.get('max')\n",
    "        print(f\"📊 Range: {min_val} - {max_val}\")\n",
    "    \n",
    "    # Criteria\n",
    "    criteria = rubric.get('criteria', [])\n",
    "    print(f\"\\n📝 Criteria ({len(criteria)}):\")\n",
    "    for i, criterion in enumerate(criteria, 1):\n",
    "        name = criterion.get('name', 'Unnamed')\n",
    "        weight = criterion.get('weight', 1.0)\n",
    "        print(f\"  {i}. {name} (weight: {weight})\")\n",
    "    \n",
    "    # Warnings\n",
    "    warnings = rubric.get('source_parse', {}).get('warnings', [])\n",
    "    if warnings:\n",
    "        print(f\"\\n⚠️  Warnings ({len(warnings)}):\")\n",
    "        for warning in warnings:\n",
    "            print(f\"  • {warning}\")\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "\n",
    "# CLI functionality (only runs when script is executed directly, not in notebook)\n",
    "if __name__ == \"__main__\" and not hasattr(__builtins__, '__IPYTHON__'):\n",
    "    import argparse, pprint\n",
    "    ap = argparse.ArgumentParser(description=\"Parse a rubric file into canonical JSON.\")\n",
    "    ap.add_argument(\"file\", help=\"Path to rubric: .txt .docx .pdf .png .jpg\")\n",
    "    ap.add_argument(\"--model\", default=\"gpt-4.1-mini\", help=\"OpenAI model (supports Structured Outputs).\")\n",
    "    args = ap.parse_args()\n",
    "\n",
    "    result = parse_rubric_file(args.file, model=args.model)\n",
    "    print(json.dumps(result, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb720c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Rubric parsed successfully!\n",
      "📊 Found 6 criteria\n",
      "📏 Scale type: categorical\n",
      "==================================================\n",
      "📋 RUBRIC: None\n",
      "==================================================\n",
      "📏 Scale Type: categorical\n",
      "📊 Levels: Excellent → Good → Fair → Poor\n",
      "\n",
      "📝 Criteria (6):\n",
      "  1. Thesis & Focus (weight: 3)\n",
      "  2. Organization (weight: 2)\n",
      "  3. Evidence & Support (weight: 3)\n",
      "  4. Analysis & Reasoning (weight: 2)\n",
      "  5. Style & Clarity (weight: 1)\n",
      "  6. Grammar & Mechanics (weight: 1)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Example usage in Jupyter notebook\n",
    "# Uncomment and modify the path below to test with your rubric file\n",
    "\n",
    "# Example 1: Parse a rubric file\n",
    "file_path = \"test_file/test_rubric.docx\"  # or .docx, .txt, .png, .jpg\n",
    "result = demo_parse_rubric(file_path)\n",
    "print_rubric_summary(result)\n",
    "\n",
    "# Example 2: Just load the functions without running\n",
    "# print(\"✅ Rubric parser functions loaded successfully!\")\n",
    "# print(\"📝 Available functions:\")\n",
    "# print(\"  • demo_parse_rubric(file_path, model='gpt-4.1-mini') - Parse a rubric file\")\n",
    "# print(\"  • print_rubric_summary(rubric) - Display formatted summary\")\n",
    "# print(\"  • parse_rubric_file(file_path, model) - Core parsing function\")\n",
    "# print(\"  • extract_text_from_file(file_path) - Extract text from various file formats\")\n",
    "# print(\"\\n💡 To use: Uncomment the example code above and provide a valid file path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56eaefdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Full JSON Result:\n",
      "==================================================\n",
      "{\n",
      "  \"title\": null,\n",
      "  \"scale\": {\n",
      "    \"type\": \"categorical\",\n",
      "    \"levels\": [\n",
      "      \"Excellent\",\n",
      "      \"Good\",\n",
      "      \"Fair\",\n",
      "      \"Poor\"\n",
      "    ]\n",
      "  },\n",
      "  \"criteria\": [\n",
      "    {\n",
      "      \"name\": \"Thesis & Focus\",\n",
      "      \"descriptor_by_level\": {\n",
      "        \"Excellent\": \"Clear, original thesis; focused throughout\",\n",
      "        \"Good\": \"Clear thesis, mostly maintained focus\",\n",
      "        \"Fair\": \"Thesis present but vague; focus drifts\",\n",
      "        \"Poor\": \"No clear thesis; unfocused\"\n",
      "      },\n",
      "      \"weight\": 3\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Organization\",\n",
      "      \"descriptor_by_level\": {\n",
      "        \"Excellent\": \"Logical structure; smooth transitions\",\n",
      "        \"Good\": \"Mostly logical; some transitions unclear\",\n",
      "        \"Fair\": \"Some organization; transitions weak\",\n",
      "        \"Poor\": \"Disorganized; lacks clear structure\"\n",
      "      },\n",
      "      \"weight\": 2\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Evidence & Support\",\n",
      "      \"descriptor_by_level\": {\n",
      "        \"Excellent\": \"Strong, relevant evidence; well-integrated\",\n",
      "        \"Good\": \"Good evidence; mostly relevant\",\n",
      "        \"Fair\": \"Limited or somewhat irrelevant evidence\",\n",
      "        \"Poor\": \"Lacks evidence or off-topic\"\n",
      "      },\n",
      "      \"weight\": 3\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Analysis & Reasoning\",\n",
      "      \"descriptor_by_level\": {\n",
      "        \"Excellent\": \"Insightful analysis; goes beyond summary\",\n",
      "        \"Good\": \"Some analysis; mostly explains evidence\",\n",
      "        \"Fair\": \"Little analysis; mostly summary\",\n",
      "        \"Poor\": \"No analysis; only summary or off-topic\"\n",
      "      },\n",
      "      \"weight\": 2\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Style & Clarity\",\n",
      "      \"descriptor_by_level\": {\n",
      "        \"Excellent\": \"Clear, engaging, and concise writing\",\n",
      "        \"Good\": \"Generally clear; minor awkwardness\",\n",
      "        \"Fair\": \"Wordy or occasionally unclear\",\n",
      "        \"Poor\": \"Hard to follow; confusing\"\n",
      "      },\n",
      "      \"weight\": 1\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Grammar & Mechanics\",\n",
      "      \"descriptor_by_level\": {\n",
      "        \"Excellent\": \"Virtually error-free; correct spelling/punctuation\",\n",
      "        \"Good\": \"Few minor errors; does not affect understanding\",\n",
      "        \"Fair\": \"Several errors; somewhat distracting\",\n",
      "        \"Poor\": \"Frequent errors; impedes understanding\"\n",
      "      },\n",
      "      \"weight\": 1\n",
      "    }\n",
      "  ],\n",
      "  \"source_parse\": {\n",
      "    \"method\": \"narrative\",\n",
      "    \"confidence\": 1.0,\n",
      "    \"warnings\": []\n",
      "  }\n",
      "}\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Print the result in JSON format\n",
    "import json\n",
    "\n",
    "# Pretty print the result as JSON\n",
    "print(\"📄 Full JSON Result:\")\n",
    "print(\"=\" * 50)\n",
    "print(json.dumps(result, indent=2, ensure_ascii=False))\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d0de35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
