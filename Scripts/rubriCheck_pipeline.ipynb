{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RubriCheck Complete Pipeline\n",
        "============================\n",
        "Integrates all three modules:\n",
        "1. Essay Preprocessor (essay_preprocessor.ipynb)\n",
        "2. Rubric Parser (rubric_parser_prompt.ipynb) \n",
        "3. Grading Engine (grading_engine.ipynb)\n",
        "\n",
        "## Usage:\n",
        "- **Command Line**: `python rubriCheck_pipeline.py --rubric path/to/rubric.pdf --essay path/to/essay.txt`\n",
        "- **Programmatic**: Use the `RubriCheckPipeline` class in your code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Any, Optional, Tuple\n",
        "from dataclasses import asdict\n",
        "\n",
        "\n",
        "# Add current directory to path for imports (Jupyter notebook compatible)\n",
        "import os\n",
        "current_dir = os.getcwd()\n",
        "if current_dir not in sys.path:\n",
        "    sys.path.append(current_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All modules imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import all three modules\n",
        "try:\n",
        "    # Import from essay_preprocessor.py\n",
        "    from essay_preprocessor import EssayPreprocessor, PreprocessOptions, ProcessedEssay\n",
        "    \n",
        "    # Import from rubric_parser_prompt.py  \n",
        "    from rubric_parser_prompt import parse_rubric_file, demo_parse_rubric\n",
        "    \n",
        "    # Import from grading_engine.py\n",
        "    from grading_engine import grade_essay, GradeSummary\n",
        "    \n",
        "    print(\"‚úÖ All modules imported successfully!\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Import error: {e}\")\n",
        "    print(\"Make sure all three Python modules are in the same directory\")\n",
        "    print(\"Available files:\")\n",
        "    import os\n",
        "    for f in os.listdir('.'):\n",
        "        if f.endswith('.py'):\n",
        "            print(f\"  - {f}\")\n",
        "    sys.exit(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RubriCheckPipeline Class\n",
        "The main pipeline class that integrates all three modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RubriCheckPipeline:\n",
        "    \"\"\"\n",
        "    Complete pipeline that integrates all three RubriCheck modules.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, api_key_file: str = r\"C:\\Users\\Leo\\AI projects\\_api.txt\"):\n",
        "        \"\"\"Initialize the pipeline with API key configuration.\"\"\"\n",
        "        self.api_key_file = api_key_file\n",
        "        self._setup_api_key()\n",
        "        \n",
        "    def _setup_api_key(self):\n",
        "        \"\"\"Set up API key from file.\"\"\"\n",
        "        try:\n",
        "            with open(self.api_key_file, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    if line.strip().startswith('rubriCheck:'):\n",
        "                        api_key = line.strip().split(':', 1)[1].strip()\n",
        "                        os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "                        print(\"‚úÖ API key loaded successfully\")\n",
        "                        return\n",
        "            raise ValueError(\"rubriCheck API key not found in file\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"‚ùå API file not found at {self.api_key_file}\")\n",
        "            sys.exit(1)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error loading API key: {e}\")\n",
        "            sys.exit(1)\n",
        "    \n",
        "    def process_essay(self, essay_path: str, options: Optional[PreprocessOptions] = None) -> ProcessedEssay:\n",
        "        \"\"\"Step 1: Process essay using essay_preprocessor module.\"\"\"\n",
        "        print(f\"üìù Processing essay: {essay_path}\")\n",
        "        \n",
        "        if not os.path.exists(essay_path):\n",
        "            raise FileNotFoundError(f\"Essay file not found: {essay_path}\")\n",
        "            \n",
        "        with open(essay_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            essay_text = f.read()\n",
        "        \n",
        "        preprocessor = EssayPreprocessor()\n",
        "        options = options or PreprocessOptions()\n",
        "        processed_essay = preprocessor.run(essay_text, options)\n",
        "        \n",
        "        print(f\"‚úÖ Essay processed: {len(processed_essay.paragraphs)} paragraphs, {processed_essay.metadata.word_count} words\")\n",
        "        return processed_essay\n",
        "    \n",
        "    def parse_rubric(self, rubric_path: str) -> Dict[str, Any]:\n",
        "        \"\"\"Step 2: Parse rubric using rubric_parser_prompt module.\"\"\"\n",
        "        print(f\"üìã Parsing rubric: {rubric_path}\")\n",
        "        \n",
        "        if not os.path.exists(rubric_path):\n",
        "            raise FileNotFoundError(f\"Rubric file not found: {rubric_path}\")\n",
        "        \n",
        "        rubric = demo_parse_rubric(rubric_path)\n",
        "        \n",
        "        if not rubric:\n",
        "            raise ValueError(\"Failed to parse rubric\")\n",
        "            \n",
        "        print(f\"‚úÖ Rubric parsed: {len(rubric.get('criteria', []))} criteria\")\n",
        "        return rubric\n",
        "    \n",
        "    def grade_essay(self, rubric: Dict[str, Any], processed_essay: ProcessedEssay, \n",
        "                  max_span_chars: int = 240) -> GradeSummary:\n",
        "        \"\"\"Step 3: Grade essay using grading_engine module.\"\"\"\n",
        "        print(\"ü§ñ Grading essay with AI...\")\n",
        "        \n",
        "        essay_paragraphs = [p.text for p in processed_essay.paragraphs if p.text.strip()]\n",
        "        converted_rubric = self._convert_rubric_format(rubric)\n",
        "        summary = grade_essay(converted_rubric, essay_paragraphs, max_span_chars)\n",
        "        \n",
        "        print(f\"‚úÖ Grading complete: {summary.numeric_score} ({summary.letter})\")\n",
        "        return summary\n",
        "    \n",
        "    def _convert_rubric_format(self, rubric: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Convert rubric from parser format to grader format.\"\"\"\n",
        "        converted = {\n",
        "            \"criteria\": [],\n",
        "            \"grading\": {\n",
        "                \"numeric\": True,\n",
        "                \"letter_bands\": [\n",
        "                    {\"min\": 90, \"max\": 100, \"letter\": \"A+\"},\n",
        "                    {\"min\": 85, \"max\": 89.99, \"letter\": \"A\"},\n",
        "                    {\"min\": 80, \"max\": 84.99, \"letter\": \"A-\"},\n",
        "                    {\"min\": 0, \"max\": 79.99, \"letter\": \"B or below\"}\n",
        "                ],\n",
        "                \"categorical_points_map\": {\"Excellent\": 4, \"Good\": 3, \"Fair\": 2, \"Poor\": 1}\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        for i, criterion in enumerate(rubric.get('criteria', [])):\n",
        "            converted_criterion = {\n",
        "                \"id\": f\"criterion_{i}\",\n",
        "                \"name\": criterion.get('name', f'Criterion {i+1}'),\n",
        "                \"descriptors\": criterion.get('descriptor_by_level', {}),\n",
        "                \"valid_levels\": rubric.get('scale', {}).get('levels', ['Excellent', 'Good', 'Fair', 'Poor']),\n",
        "                \"weight\": criterion.get('weight', 1.0) / sum(c.get('weight', 1.0) for c in rubric.get('criteria', [])),\n",
        "                \"level_scale_note\": \" ‚Üí \".join(rubric.get('scale', {}).get('levels', []))\n",
        "            }\n",
        "            converted[\"criteria\"].append(converted_criterion)\n",
        "        \n",
        "        return converted\n",
        "    \n",
        "    def run_complete_pipeline(self, rubric_path: str, essay_path: str, \n",
        "                            output_path: Optional[str] = None,\n",
        "                            essay_options: Optional[PreprocessOptions] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Run the complete pipeline: essay preprocessing ‚Üí rubric parsing ‚Üí AI grading.\"\"\"\n",
        "        print(\"üöÄ Starting RubriCheck Complete Pipeline\")\n",
        "        print(\"=\" * 50)\n",
        "        \n",
        "        try:\n",
        "            # Step 1: Process essay\n",
        "            processed_essay = self.process_essay(essay_path, essay_options)\n",
        "            \n",
        "            # Step 2: Parse rubric\n",
        "            rubric = self.parse_rubric(rubric_path)\n",
        "            \n",
        "            # Step 3: Grade essay\n",
        "            grade_summary = self.grade_essay(rubric, processed_essay)\n",
        "            \n",
        "            # Compile results\n",
        "            results = {\n",
        "                \"pipeline_info\": {\n",
        "                    \"rubric_file\": rubric_path,\n",
        "                    \"essay_file\": essay_path,\n",
        "                    \"timestamp\": str(Path().cwd()),\n",
        "                    \"version\": \"1.0\"\n",
        "                },\n",
        "                \"essay_metadata\": asdict(processed_essay.metadata),\n",
        "                \"rubric_info\": {\n",
        "                    \"title\": rubric.get('title'),\n",
        "                    \"scale_type\": rubric.get('scale', {}).get('type'),\n",
        "                    \"criteria_count\": len(rubric.get('criteria', [])),\n",
        "                    \"source_parse\": rubric.get('source_parse', {})\n",
        "                },\n",
        "                \"grading_results\": {\n",
        "                    \"per_criterion\": [asdict(r) for r in grade_summary.per_criterion],\n",
        "                    \"numeric_score\": grade_summary.numeric_score,\n",
        "                    \"letter_grade\": grade_summary.letter,\n",
        "                    \"categorical_points\": grade_summary.categorical_points,\n",
        "                    \"reliability_flags\": grade_summary.notes\n",
        "                },\n",
        "                \"warnings\": processed_essay.warnings + rubric.get('source_parse', {}).get('warnings', [])\n",
        "            }\n",
        "            \n",
        "            # Save results if output path provided\n",
        "            if output_path:\n",
        "                with open(output_path, 'w', encoding='utf-8') as f:\n",
        "                    json.dump(results, f, indent=2, ensure_ascii=False)\n",
        "                print(f\"üíæ Results saved to: {output_path}\")\n",
        "            \n",
        "            print(\"\\nüéâ Pipeline completed successfully!\")\n",
        "            return results\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Pipeline failed: {e}\")\n",
        "            raise\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: Essay Processing\n",
        "Process essays using the essay_preprocessor module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This method is now part of the RubriCheckPipeline class above\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Rubric Parsing\n",
        "Parse rubrics using the rubric_parser_prompt module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This method is now part of the RubriCheckPipeline class above\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: AI Grading\n",
        "Grade essays using the grading_engine module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This method is now part of the RubriCheckPipeline class above\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Helper Methods\n",
        "Utility methods for the pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This method is now part of the RubriCheckPipeline class above\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Complete Pipeline\n",
        "Run the complete pipeline: essay preprocessing ‚Üí rubric parsing ‚Üí AI grading.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This method is now part of the RubriCheckPipeline class above\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Usage Examples\n",
        "\n",
        "### Example 1: Basic Pipeline Usage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî¨ Example: Basic Pipeline Usage\n",
            "========================================\n",
            "‚úÖ API key loaded successfully\n",
            "üìù Created sample essay: test_essay.txt\n",
            "‚ùå Error: 'RubriCheckPipeline' object has no attribute 'run_complete_pipeline'\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Basic Pipeline Usage\n",
        "def example_basic_usage():\n",
        "    \"\"\"Basic example of using the pipeline.\"\"\"\n",
        "    print(\"üî¨ Example: Basic Pipeline Usage\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    # Initialize pipeline\n",
        "    pipeline = RubriCheckPipeline()\n",
        "    \n",
        "    # Example file paths (adjust these to your actual files)\n",
        "    rubric_path = \"test_file/test_rubric.docx\"\n",
        "    essay_path = \"test_essay.txt\"  # You'll need to create this\n",
        "    \n",
        "    # Create a sample essay if it doesn't exist\n",
        "    if not os.path.exists(essay_path):\n",
        "        sample_essay = \"\"\"\n",
        "        This essay argues that renewable energy is essential to national security by reducing dependence on volatile fuel markets.\n",
        "        \n",
        "        Several reports show countries with higher renewable portfolios experience less price shock; however, grid stability challenges remain.\n",
        "        \n",
        "        Opponents claim costs are prohibitive; this essay demonstrates recent cost curves and policy mechanisms that offset initial investment.\n",
        "        \"\"\"\n",
        "        with open(essay_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(sample_essay)\n",
        "        print(f\"üìù Created sample essay: {essay_path}\")\n",
        "    \n",
        "    try:\n",
        "        # Run complete pipeline\n",
        "        results = pipeline.run_complete_pipeline(\n",
        "            rubric_path=rubric_path,\n",
        "            essay_path=essay_path,\n",
        "            output_path=\"example_results.json\"\n",
        "        )\n",
        "        \n",
        "        # Print results\n",
        "        print(\"\\nüìä Results:\")\n",
        "        grading = results[\"grading_results\"]\n",
        "        print(f\"Score: {grading['numeric_score']} ({grading['letter_grade']})\")\n",
        "        \n",
        "        return results\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        return None\n",
        "\n",
        "# Run the basic example\n",
        "example_basic_usage()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Step-by-Step Execution\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Step-by-Step Execution\n",
        "def example_step_by_step():\n",
        "    \"\"\"Example showing step-by-step pipeline execution.\"\"\"\n",
        "    print(\"üîç Example: Step-by-Step Execution\")\n",
        "    print(\"=\" * 35)\n",
        "    \n",
        "    pipeline = RubriCheckPipeline()\n",
        "    \n",
        "    try:\n",
        "        # Step 1: Process essay\n",
        "        print(\"Step 1: Processing essay...\")\n",
        "        processed_essay = pipeline.process_essay(\"test_essay.txt\")\n",
        "        print(f\"   - {len(processed_essay.paragraphs)} paragraphs\")\n",
        "        print(f\"   - {processed_essay.metadata.word_count} words\")\n",
        "        print(f\"   - Language: {processed_essay.metadata.language_detected}\")\n",
        "        \n",
        "        # Step 2: Parse rubric\n",
        "        print(\"\\nStep 2: Parsing rubric...\")\n",
        "        rubric = pipeline.parse_rubric(\"test_file/test_rubric.docx\")\n",
        "        print(f\"   - {len(rubric.get('criteria', []))} criteria\")\n",
        "        print(f\"   - Scale type: {rubric.get('scale', {}).get('type')}\")\n",
        "        \n",
        "        # Step 3: Grade essay\n",
        "        print(\"\\nStep 3: Grading essay...\")\n",
        "        grade_summary = pipeline.grade_essay(rubric, processed_essay)\n",
        "        print(f\"   - Score: {grade_summary.numeric_score}\")\n",
        "        print(f\"   - Letter: {grade_summary.letter}\")\n",
        "        print(f\"   - Criteria evaluated: {len(grade_summary.per_criterion)}\")\n",
        "        \n",
        "        # Show detailed results\n",
        "        print(\"\\nüìã Detailed Results:\")\n",
        "        for i, criterion in enumerate(grade_summary.per_criterion, 1):\n",
        "            print(f\"   {i}. {criterion.criterion_id}: {criterion.level}\")\n",
        "            if criterion.justification:\n",
        "                print(f\"      Justification: {criterion.justification[:80]}...\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "\n",
        "# Run the step-by-step example\n",
        "# example_step_by_step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook provides the complete RubriCheck pipeline that integrates all three modules:\n",
        "\n",
        "1. **Essay Preprocessor** (`essay_preprocessor.ipynb`) - Processes and structures student essays\n",
        "2. **Rubric Parser** (`rubric_parser_prompt.ipynb`) - Extracts and parses rubric documents  \n",
        "3. **Grading Engine** (`grading_engine.ipynb`) - Grades essays using AI against rubrics\n",
        "\n",
        "### Key Features:\n",
        "- **Complete Integration**: All three modules work together seamlessly\n",
        "- **Flexible Usage**: Can run complete pipeline or individual steps\n",
        "- **Multiple Formats**: Supports various file formats (PDF, DOCX, TXT, images)\n",
        "- **Comprehensive Results**: Detailed grading with justifications and suggestions\n",
        "- **Error Handling**: Robust error handling and informative messages\n",
        "\n",
        "### Usage:\n",
        "- **Programmatic**: Use the `RubriCheckPipeline` class in your code\n",
        "- **Interactive**: Run individual cells to test specific functionality\n",
        "- **Examples**: Uncomment the example functions to see the pipeline in action\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
